{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of the project is to learn the mapping from polar coordinates to a a discrete 10x10 grid of cells in the plane, using a neural network. \n",
        "\n",
        "The supervised dataset is given to you in the form of a generator (to be considered as a black box).\n",
        "\n",
        "The model must achieve an accuracy of 95%, and it will be evaluated in a way **inversely proportional to the number of its parameters: the smaller, the better.**\n",
        "\n",
        "**WARNING**: Any solution taking advantage of meta-knowledge about the generator will be automatically rejected."
      ],
      "metadata": {
        "id": "Zw_326KLT9dF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ynz-4_4cFmbJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, concatenate\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the generator. It returns triples of the form ((theta,rho),out) where (theta,rho) are the polar coordinates of a point in the first quadrant of the plane, and out is a 10x10 map with \"1\" in the cell corresponding to the point position, and \"0\" everywhere else.\n",
        "\n",
        "By setting flat=True, the resulting map is flattened into a vector with a single dimension 100. You can use this variant, if you wish. "
      ],
      "metadata": {
        "id": "iA01pkKbUt7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def polar_generator(batchsize,grid=(10,10),noise=.002,flat=False):\n",
        "  while True:\n",
        "    x = np.random.rand(batchsize)\n",
        "    y = np.random.rand(batchsize)\n",
        "    out = np.zeros((batchsize,grid[0],grid[1]))\n",
        "    xc = (x*grid[0]).astype(int)\n",
        "    yc = (y*grid[1]).astype(int)\n",
        "    for b in range(batchsize):\n",
        "      out[b,xc[b],yc[b]] = 1\n",
        "    #compute rho and theta and add some noise\n",
        "    rho = np.sqrt(x**2+y**2) + np.random.normal(scale=noise)\n",
        "    theta = np.arctan(y/np.maximum(x,.00001)) + np.random.normal(scale=noise)\n",
        "    if flat:\n",
        "      out = np.reshape(out,(batchsize,grid[0]*grid[1]))\n",
        "    yield ((theta,rho),out)"
      ],
      "metadata": {
        "id": "DsA1GqAeWAdo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create an instance of the generator on a grid with dimension 3x4"
      ],
      "metadata": {
        "id": "ZF-jlaqAWc2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g1,g2 = 3,4\n",
        "gen = polar_generator(1,grid=(g1,g2),noise=0.0)"
      ],
      "metadata": {
        "id": "Ov3rXaLVHDCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now let's see a few samples."
      ],
      "metadata": {
        "id": "b4hntQtSWjPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(theta,rho),maps = next(gen)\n",
        "for i,map in enumerate(maps):\n",
        "  #let us compute the cartesian coordinates\n",
        "  x = np.cos(theta[i])*rho[i]\n",
        "  y = np.sin(theta[i])*rho[i]\n",
        "  print(\"x coordinate (row): {}\".format(int(x*g1)))\n",
        "  print(\"y coordinate (col): {}\".format(int(y*g2)))\n",
        "  print(\"map:\")\n",
        "  print(np.reshape(map,(g1,g2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PM7R8ZZZHN7p",
        "outputId": "92c37cd5-6f7e-4d40-d57c-c4a132a739d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x coordinate (row): 0\n",
            "y coordinate (col): 1\n",
            "map:\n",
            "[[0. 1. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise: add noise to the generator, and check the effect on the \"ground truth\"."
      ],
      "metadata": {
        "id": "NTY5fu8Hg7RE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What to deliver\n",
        "\n",
        "For the purposes of the project you are supposed to work with the **default 10x10 grid, and the default noise=.002**\n",
        "\n",
        "The generator must be treatead as a black box, do not tweak it, and do not exploit its semantics that is supposed to be unknown. You are allowed to work with the \"flat\" modality, if you prefer so.\n",
        "\n",
        "You need to:\n",
        "1.   define an accuracy function (take inspiration from the code of the previous cell)\n",
        "2.   define a neural network taking in input theta and rho, and returning out\n",
        "3. measure the network's accuracy that must be above 95% (accuracy must be evaluated over at least 20000 samples)\n",
        "4. tune the network trying to decrease as much as possible the numer of parameters, preserving an accuracy above 95%. Only your best network must be delivered.\n",
        "\n",
        "You must deliver a SINGLE notebook working on colab, containing the code of the network, its summary, the training history, the code for the accurary metric and its evaluation on the network.\n",
        "\n",
        "**N.B.** The accuracy must be above 95% but apart from that it does not influence the evaluation. You score will only depend on the number of parameters: the lower, the better.\n",
        "\n",
        "#Good work!\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jj4akvA24maJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.experimental import AdamW\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import LeakyReLU, Dropout"
      ],
      "metadata": {
        "id": "_pjE0IGoyVW-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_unit(n: int):\n",
        "  res = 1\n",
        "  while(2^res < n):\n",
        "    res += 1\n",
        "  return res"
      ],
      "metadata": {
        "id": "0oiK7MDZpVaq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_network(output_shape=(10,10)):\n",
        "  #theta, ro = theta_ro\n",
        "  out1, out2 = output_shape\n",
        "  out_area = out1 * out2\n",
        "\n",
        "  input1 = Input(shape=(1,))\n",
        "  input2 = Input(shape=(1,))\n",
        "  merged = concatenate([input1, input2])\n",
        "\n",
        "  d1 = Dense(48)(merged)\n",
        "  d1 = LeakyReLU()(d1)\n",
        "  d1 = Dense(32)(d1)\n",
        "  d1 = LeakyReLU()(d1)\n",
        "  d1 = Dropout(0.1)(d1)\n",
        "  d2 = Dense(out_area, activation='softmax')(d1)\n",
        "  \n",
        "  model = Model(inputs=[input1, input2], outputs=d2)\n",
        "  print(model.summary())\n",
        "  return model\n",
        "\n",
        "# def get_network(output_shape=(10,10)):\n",
        "#   out1, out2 = output_shape\n",
        "#   out_area = out1 * out2\n",
        "\n",
        "#   input1 = Input(shape=(1,))\n",
        "#   input2 = Input(shape=(1,))\n",
        "\n",
        "#   d1 = Dense(np.rint(np.sqrt(out_area)), activation='relu')(input1)\n",
        "#   d2 = Dense(np.rint(np.sqrt(out_area)), activation='relu')(input2)\n",
        "#   merged = concatenate([d1, d2])\n",
        "\n",
        "#   #inputs = concatenate([theta, ro])\n",
        "#   #x = Input(shape=input_shape)\n",
        "#   d1 = Dense(np.rint(np.sqrt(out_area)), activation='relu')(merged)\n",
        "#   #d1 = Dense(out_area//2, activation='relu')(merged)\n",
        "#   d2 = Dense(out_area, activation='softmax')(d1)\n",
        "  \n",
        "#   model = Model(inputs=[input1, input2], outputs=d2)\n",
        "#   print(model.summary())\n",
        "#   return model\n",
        "\n",
        "# def get_network(output_shape=(10,10)):\n",
        "#   out1, out2 = output_shape\n",
        "#   out_area = out1 * out2\n",
        "\n",
        "#   input1 = Input(shape=(1,))\n",
        "#   input2 = Input(shape=(1,))\n",
        "\n",
        "#   u1 = 32\n",
        "\n",
        "#   d1 = Dense(u1)(input1)\n",
        "#   d1 = LeakyReLU()(d1)\n",
        "#   d2 = Dense(u1)(input2)\n",
        "#   d2 = LeakyReLU()(d2)\n",
        "#   merged = concatenate([d1, d2])\n",
        "\n",
        "#   u2 = 128\n",
        "#   d3 = Dense(u2)(merged)\n",
        "#   d3 = LeakyReLU()(d3)\n",
        "#   d3 = Dropout(0.2)(d3)\n",
        "\n",
        "#   d3 = Dense(u2*2)(d3)\n",
        "#   d3 = LeakyReLU()(d3)\n",
        "#   d4 = Dense(out_area, activation='softmax')(d3)\n",
        "  \n",
        "#   model = Model(inputs=[input1, input2], outputs=d4)\n",
        "#   print(model.summary())\n",
        "#   return model\n"
      ],
      "metadata": {
        "id": "iQlY1kqcypBe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = get_network(output_shape=(10,10))\n",
        "learning_rate = 4e-3\n",
        "weight_decay = 1e-4\n",
        "\n",
        "net.compile(\n",
        "    optimizer=Adam(learning_rate=learning_rate),\n",
        "    loss=CategoricalCrossentropy()\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT52b_0w1Z4R",
        "outputId": "eaa5634a-53e2-45b2-d3aa-1cf7ec06e26e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 2)            0           ['input_5[0][0]',                \n",
            "                                                                  'input_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 48)           144         ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " leaky_re_lu_4 (LeakyReLU)      (None, 48)           0           ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 32)           1568        ['leaky_re_lu_4[0][0]']          \n",
            "                                                                                                  \n",
            " leaky_re_lu_5 (LeakyReLU)      (None, 32)           0           ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 32)           0           ['leaky_re_lu_5[0][0]']          \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 100)          3300        ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,012\n",
            "Trainable params: 5,012\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 2048\n",
        "\n",
        "callback = EarlyStopping(monitor=\"loss\",\n",
        "    min_delta=0.0005,\n",
        "    patience=20,\n",
        "    restore_best_weights=True)\n",
        "\n",
        "# history = net.fit_generator(polar_generator(BATCH_SIZE), steps_per_epoch=1000, epochs=10)\n",
        "history = net.fit(polar_generator(BATCH_SIZE, flat=True), batch_size = BATCH_SIZE, steps_per_epoch=1000, epochs=1000, callbacks=[callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCILuGA53OgB",
        "outputId": "925dc6a3-ce07-4bfc-b806-21920d480b83"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "1000/1000 [==============================] - 13s 12ms/step - loss: 1.2156\n",
            "Epoch 2/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.6206\n",
            "Epoch 3/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.5430\n",
            "Epoch 4/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.5002\n",
            "Epoch 5/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4703\n",
            "Epoch 6/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4509\n",
            "Epoch 7/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4342\n",
            "Epoch 8/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4237\n",
            "Epoch 9/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4146\n",
            "Epoch 10/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4034\n",
            "Epoch 11/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3965\n",
            "Epoch 12/1000\n",
            "1000/1000 [==============================] - 14s 14ms/step - loss: 0.3881\n",
            "Epoch 13/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3810\n",
            "Epoch 14/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3715\n",
            "Epoch 15/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3704\n",
            "Epoch 16/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3630\n",
            "Epoch 17/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3610\n",
            "Epoch 18/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3536\n",
            "Epoch 19/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3523\n",
            "Epoch 20/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3494\n",
            "Epoch 21/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3409\n",
            "Epoch 22/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3409\n",
            "Epoch 23/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3396\n",
            "Epoch 24/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3335\n",
            "Epoch 25/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3323\n",
            "Epoch 26/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3314\n",
            "Epoch 27/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3302\n",
            "Epoch 28/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3273\n",
            "Epoch 29/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3232\n",
            "Epoch 30/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3225\n",
            "Epoch 31/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3209\n",
            "Epoch 32/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3180\n",
            "Epoch 33/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3181\n",
            "Epoch 34/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3142\n",
            "Epoch 35/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3117\n",
            "Epoch 36/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3125\n",
            "Epoch 37/1000\n",
            "1000/1000 [==============================] - 14s 14ms/step - loss: 0.3086\n",
            "Epoch 38/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3061\n",
            "Epoch 39/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3063\n",
            "Epoch 40/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3040\n",
            "Epoch 41/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3010\n",
            "Epoch 42/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3015\n",
            "Epoch 43/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3000\n",
            "Epoch 44/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2958\n",
            "Epoch 45/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2949\n",
            "Epoch 46/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2924\n",
            "Epoch 47/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2934\n",
            "Epoch 48/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2914\n",
            "Epoch 49/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2896\n",
            "Epoch 50/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2853\n",
            "Epoch 51/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2871\n",
            "Epoch 52/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2835\n",
            "Epoch 53/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2830\n",
            "Epoch 54/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2812\n",
            "Epoch 55/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2796\n",
            "Epoch 56/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2761\n",
            "Epoch 57/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2769\n",
            "Epoch 58/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2747\n",
            "Epoch 59/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2720\n",
            "Epoch 60/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2728\n",
            "Epoch 61/1000\n",
            "1000/1000 [==============================] - 14s 14ms/step - loss: 0.2703\n",
            "Epoch 62/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2670\n",
            "Epoch 63/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2698\n",
            "Epoch 64/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2668\n",
            "Epoch 65/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2669\n",
            "Epoch 66/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2636\n",
            "Epoch 67/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2631\n",
            "Epoch 68/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2631\n",
            "Epoch 69/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2590\n",
            "Epoch 70/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2602\n",
            "Epoch 71/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2601\n",
            "Epoch 72/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2558\n",
            "Epoch 73/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2580\n",
            "Epoch 74/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2601\n",
            "Epoch 75/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2528\n",
            "Epoch 76/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2569\n",
            "Epoch 77/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2538\n",
            "Epoch 78/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2530\n",
            "Epoch 79/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2510\n",
            "Epoch 80/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2533\n",
            "Epoch 81/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2511\n",
            "Epoch 82/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2500\n",
            "Epoch 83/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2484\n",
            "Epoch 84/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2501\n",
            "Epoch 85/1000\n",
            "1000/1000 [==============================] - 14s 14ms/step - loss: 0.2505\n",
            "Epoch 86/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2478\n",
            "Epoch 87/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2450\n",
            "Epoch 88/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2460\n",
            "Epoch 89/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2452\n",
            "Epoch 90/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2464\n",
            "Epoch 91/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2448\n",
            "Epoch 92/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2416\n",
            "Epoch 93/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2392\n",
            "Epoch 94/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2459\n",
            "Epoch 95/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2419\n",
            "Epoch 96/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2420\n",
            "Epoch 97/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2390\n",
            "Epoch 98/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2397\n",
            "Epoch 99/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2372\n",
            "Epoch 100/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2375\n",
            "Epoch 101/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2342\n",
            "Epoch 102/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2408\n",
            "Epoch 103/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2344\n",
            "Epoch 104/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2336\n",
            "Epoch 105/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2328\n",
            "Epoch 106/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2337\n",
            "Epoch 107/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2350\n",
            "Epoch 108/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2335\n",
            "Epoch 109/1000\n",
            "1000/1000 [==============================] - 14s 14ms/step - loss: 0.2343\n",
            "Epoch 110/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2315\n",
            "Epoch 111/1000\n",
            "1000/1000 [==============================] - 13s 12ms/step - loss: 0.2332\n",
            "Epoch 112/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2298\n",
            "Epoch 113/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2311\n",
            "Epoch 114/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2285\n",
            "Epoch 115/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2288\n",
            "Epoch 116/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2266\n",
            "Epoch 117/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2252\n",
            "Epoch 118/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2302\n",
            "Epoch 119/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2272\n",
            "Epoch 120/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2270\n",
            "Epoch 121/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2253\n",
            "Epoch 122/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2279\n",
            "Epoch 123/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2256\n",
            "Epoch 124/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2242\n",
            "Epoch 125/1000\n",
            "1000/1000 [==============================] - 12s 13ms/step - loss: 0.2271\n",
            "Epoch 126/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2249\n",
            "Epoch 127/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2228\n",
            "Epoch 128/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2256\n",
            "Epoch 129/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2206\n",
            "Epoch 130/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2213\n",
            "Epoch 131/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2235\n",
            "Epoch 132/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2204\n",
            "Epoch 133/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2206\n",
            "Epoch 134/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2173\n",
            "Epoch 135/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2189\n",
            "Epoch 136/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2163\n",
            "Epoch 137/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2222\n",
            "Epoch 138/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2190\n",
            "Epoch 139/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2157\n",
            "Epoch 140/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2174\n",
            "Epoch 141/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2193\n",
            "Epoch 142/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2169\n",
            "Epoch 143/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2183\n",
            "Epoch 144/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2164\n",
            "Epoch 145/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2167\n",
            "Epoch 146/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2176\n",
            "Epoch 147/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2127\n",
            "Epoch 148/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2160\n",
            "Epoch 149/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2154\n",
            "Epoch 150/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2146\n",
            "Epoch 151/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2145\n",
            "Epoch 152/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2152\n",
            "Epoch 153/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2159\n",
            "Epoch 154/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2119\n",
            "Epoch 155/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2120\n",
            "Epoch 156/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2126\n",
            "Epoch 157/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2142\n",
            "Epoch 158/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2151\n",
            "Epoch 159/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2131\n",
            "Epoch 160/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2122\n",
            "Epoch 161/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2135\n",
            "Epoch 162/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2096\n",
            "Epoch 163/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2107\n",
            "Epoch 164/1000\n",
            "1000/1000 [==============================] - 14s 14ms/step - loss: 0.2095\n",
            "Epoch 165/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2122\n",
            "Epoch 166/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2067\n",
            "Epoch 167/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2082\n",
            "Epoch 168/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2087\n",
            "Epoch 169/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2107\n",
            "Epoch 170/1000\n",
            "1000/1000 [==============================] - 13s 12ms/step - loss: 0.2064\n",
            "Epoch 171/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2097\n",
            "Epoch 172/1000\n",
            "1000/1000 [==============================] - 14s 14ms/step - loss: 0.2082\n",
            "Epoch 173/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2089\n",
            "Epoch 174/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2054\n",
            "Epoch 175/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2113\n",
            "Epoch 176/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2056\n",
            "Epoch 177/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2068\n",
            "Epoch 178/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2089\n",
            "Epoch 179/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2033\n",
            "Epoch 180/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2049\n",
            "Epoch 181/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2046\n",
            "Epoch 182/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2036\n",
            "Epoch 183/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2058\n",
            "Epoch 184/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2077\n",
            "Epoch 185/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2017\n",
            "Epoch 186/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2034\n",
            "Epoch 187/1000\n",
            "1000/1000 [==============================] - 14s 14ms/step - loss: 0.2012\n",
            "Epoch 188/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2074\n",
            "Epoch 189/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2023\n",
            "Epoch 190/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2025\n",
            "Epoch 191/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2035\n",
            "Epoch 192/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2038\n",
            "Epoch 193/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2014\n",
            "Epoch 194/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2046\n",
            "Epoch 195/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2022\n",
            "Epoch 196/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2051\n",
            "Epoch 197/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1983\n",
            "Epoch 198/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1990\n",
            "Epoch 199/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2010\n",
            "Epoch 200/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1999\n",
            "Epoch 201/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1997\n",
            "Epoch 202/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2005\n",
            "Epoch 203/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1991\n",
            "Epoch 204/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2004\n",
            "Epoch 205/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1969\n",
            "Epoch 206/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1988\n",
            "Epoch 207/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1994\n",
            "Epoch 208/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1990\n",
            "Epoch 209/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1966\n",
            "Epoch 210/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1964\n",
            "Epoch 211/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1998\n",
            "Epoch 212/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1987\n",
            "Epoch 213/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1973\n",
            "Epoch 214/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1974\n",
            "Epoch 215/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1924\n",
            "Epoch 216/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1994\n",
            "Epoch 217/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1989\n",
            "Epoch 218/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1937\n",
            "Epoch 219/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1935\n",
            "Epoch 220/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1967\n",
            "Epoch 221/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1944\n",
            "Epoch 222/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1966\n",
            "Epoch 223/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1926\n",
            "Epoch 224/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1938\n",
            "Epoch 225/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1937\n",
            "Epoch 226/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1923\n",
            "Epoch 227/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1944\n",
            "Epoch 228/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1959\n",
            "Epoch 229/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1932\n",
            "Epoch 230/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1925\n",
            "Epoch 231/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1918\n",
            "Epoch 232/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1901\n",
            "Epoch 233/1000\n",
            "1000/1000 [==============================] - 14s 14ms/step - loss: 0.1922\n",
            "Epoch 234/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1894\n",
            "Epoch 235/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1933\n",
            "Epoch 236/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1939\n",
            "Epoch 237/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1933\n",
            "Epoch 238/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1912\n",
            "Epoch 239/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1874\n",
            "Epoch 240/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1898\n",
            "Epoch 241/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1888\n",
            "Epoch 242/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1924\n",
            "Epoch 243/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1915\n",
            "Epoch 244/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1896\n",
            "Epoch 245/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1898\n",
            "Epoch 246/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1915\n",
            "Epoch 247/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1886\n",
            "Epoch 248/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1879\n",
            "Epoch 249/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1890\n",
            "Epoch 250/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1884\n",
            "Epoch 251/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1918\n",
            "Epoch 252/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1873\n",
            "Epoch 253/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1903\n",
            "Epoch 254/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1913\n",
            "Epoch 255/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1856\n",
            "Epoch 256/1000\n",
            "1000/1000 [==============================] - 14s 14ms/step - loss: 0.1886\n",
            "Epoch 257/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1835\n",
            "Epoch 258/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1862\n",
            "Epoch 259/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1881\n",
            "Epoch 260/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1863\n",
            "Epoch 261/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1869\n",
            "Epoch 262/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1851\n",
            "Epoch 263/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1856\n",
            "Epoch 264/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1857\n",
            "Epoch 265/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1854\n",
            "Epoch 266/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1886\n",
            "Epoch 267/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1840\n",
            "Epoch 268/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1844\n",
            "Epoch 269/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1874\n",
            "Epoch 270/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1840\n",
            "Epoch 271/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1867\n",
            "Epoch 272/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1855\n",
            "Epoch 273/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1841\n",
            "Epoch 274/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1850\n",
            "Epoch 275/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1869\n",
            "Epoch 276/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1847\n",
            "Epoch 277/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(net, gen, sample=1000):\n",
        "  (theta,rho),maps = next(gen)\n",
        "  pred = net.predict((theta,rho))\n",
        "\n",
        "  ok = 0\n",
        "  for i in range(sample):\n",
        "    if np.argmax(maps[i]) == np.argmax(pred[i]):\n",
        "      ok += 1\n",
        "\n",
        "  return ok*100/sample"
      ],
      "metadata": {
        "id": "5qz1LZxgAt3c"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy2(net, gen, sample=1000, shape=(10,10)):\n",
        "  (theta,rho),maps = next(gen)\n",
        "  pred = net.predict((theta,rho))\n",
        "\n",
        "  accuracy = 0\n",
        "  \n",
        "  for i in range(sample):\n",
        "    max_true = np.argmax(maps[i])\n",
        "    x_true = max_true//10\n",
        "    y_true = max_true%10\n",
        "    \n",
        "    max_pred = np.argmax(pred[i])\n",
        "    x_pred = max_pred//10\n",
        "    y_pred = max_pred%10\n",
        "\n",
        "    accuracy += (1 - (np.square(x_true - x_pred) + np.square(y_true - y_pred)) / (shape[0]*np.sqrt(2)))\n",
        "\n",
        "  return accuracy*100/sample"
      ],
      "metadata": {
        "id": "5CSwXIPqPOEr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = 50000\n",
        "get_accuracy(net, polar_generator(sample, grid=(10,10), flat=True), sample=sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT4GxEF5S3B8",
        "outputId": "42459dd5-0752-4245-f2c5-e4a66232d7aa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 3s 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95.422"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_accuracy2(net, polar_generator(sample, grid=(10,10), flat=True), sample=sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9Px1BA_EKuU",
        "outputId": "5501de33-16e6-4b27-b24b-9a5e8c75e6c0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 2s 1ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99.70485362953092"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}