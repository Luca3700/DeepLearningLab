{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSWSrJ-iTazX"
      },
      "source": [
        "# Generating Images with Generative Adversarial Networks (GANs)\n",
        "\n",
        "The purpose of the project is to test the ability of Generative Adversial Networks (GANs) in generating realistic-looking images. \n",
        "\n",
        "## Dataset\n",
        "\n",
        "The dataset used will be FashionMNIST. It contains low resolution ($28 \\times 28$) grey-scale images representing different kind of clothes. The dataset is available on keras and accessable in $\\texttt{tf.keras.datasets.fashion\\_mnist}$. Note that the pixel values for the images are initially in the interval $[0, 255]$. It is required to normalize them since all of the algorithm we will use require them to be in that format. To be fair, you will find the dataset already normalized, do not modify that part of the code.\n",
        "\n",
        "## Metrics\n",
        "\n",
        "Measuring the quality of newly generated images is a non-trivial task. Indeed, there is no label associated to each image, and thus it is impossible to measure the quality image-by-image. For that reason, common metrics uses statistical consideration on a generated dataset to test how well the network recovered the statistics of the original data. One of the most common is the Fr√©chet Inception Distance (FID). The idea of FID is that in a realistic-looking dataset of images, the statistics of the activation of the last hidden layer in a well-trained classificator should be similar to that of a dataset containing real images. Specifically, regarding FID, the Inception-v3 network is used as a classificator. A real dataset $\\mathbb{D}_r$ and a generated dataset $\\mathbb{D}_g$ are processed by the network, and the activation of the last hidden layer has mean and variance $(\\mu_r, \\Sigma_r)$, $(\\mu_g, \\Sigma_g)$ respectively. Then, FID is computed as:\n",
        "\n",
        "$$\n",
        "    FID(\\mathbb{D}_r, \\mathbb{D}_g) = || \\mu_r - \\mu_g ||^2 + Tr(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\ast \\Sigma_g)^{\\frac{1}{2}}) \n",
        "$$\n",
        "\n",
        "A Python implementation of FID can be found in the file $\\texttt{fid.py}$ that you find attached on Virtuale. Its usage is very simple, just generate $10k$ fake images with your GAN, and with the command $\\texttt{fid.get\\_fid(x\\_test, x\\_gen)}$, where $\\texttt{x\\_test}$ is the test set, containing $10k$ real images, you get the value for the FID of your network. Remember that, when passed through that function, $\\texttt{x\\_gen}$ **must** be a dataset of $10k$ images, in the interval $[0, 1]$. The number of $10k$ images is fundamental, since the value of FID strongly depends on the number of input images.\n",
        "\n",
        "## Limitations\n",
        "\n",
        "You are required to implement a vanilla Generative Adversarial Network (GAN), not a variant of it (e.g. PixelGAN, CycleGAN, ... are **not** accepted). The maximum number of parameters is *15 million*, and every pre-trained network can be used as an add-on (the number of parameters for pre-trained network does not count). Clearly, only the training set can be used to train the network, no additional images (Data Augmentation is ok)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aaubgTXbTazn"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceos7zgTTazs"
      },
      "source": [
        "The images are normalized in $[0, 1]$. For simplicity, images are padded to have dimension $32 \\times 32$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Es5LDnq0Tazv",
        "outputId": "2e8cf54b-6ee3-4678-f491-f3608421a372"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "Training shape: (60000, 32, 32, 1), Training pixel values: (0.0, 1.0)\n",
            "Test shape: (10000, 32, 32, 1), Test pixel values: (0.0, 1.0)\n"
          ]
        }
      ],
      "source": [
        "# Load the data. Note that the labels y_train and y_test are not loaded since not required.\n",
        "(x_train, _), (x_test, _) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize and pad the datasets\n",
        "x_train = np.pad(x_train, ((0,0), (2,2), (2,2)))\n",
        "x_train = np.reshape(x_train, x_train.shape + (1, ))\n",
        "x_train = x_train / 255.\n",
        "\n",
        "x_test = np.pad(x_test, ((0,0), (2,2), (2,2)))\n",
        "x_test = np.reshape(x_test, x_test.shape + (1, ))\n",
        "x_test = x_test / 255.\n",
        "\n",
        "print(f\"Training shape: {x_train.shape}, Training pixel values: {x_train.min(), x_train.max()}\")\n",
        "print(f\"Test shape: {x_test.shape}, Test pixel values: {x_test.min(), x_test.max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRRpm4TkTazy"
      },
      "source": [
        "Now, we import the functions for the computation of the FID, and we test that FID(x_train, x_test) is low.\n",
        "\n",
        "_Note: Computing the FID function requires some minutes. Consequently, it is suggested to comment this cell after you tested once, to reduce the execution time of the notebook. To speed-up the process, after a first use, the function will generate a file containing the value of the activations of the test set, so that it does not have to compute it again every time._ \n",
        "\n",
        "**Remember that, when you use the FID function, the first input MUST be the test set, while the second will be the generated images set.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dPZU1oiTaz1",
        "outputId": "a409e65f-c760-4475-bb6f-cdf2155a74be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 4s 0us/step\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Do not modify this code. This is just for utilities.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "# prepare the inception v3 model\n",
        "model = InceptionV3(include_top=False, pooling='avg', input_shape=(299, 299, 3), weights='imagenet')\n",
        "\n",
        "def get_inception_activations(inps, batch_size=100):\n",
        "    \"\"\"\n",
        "    Compute the activation for the model Inception v3 for a given input 'inps'.\n",
        "\n",
        "    Note: inps is assumed to be normalized in [0, 1].\n",
        "    \"\"\"\n",
        "    n_batches = inps.shape[0] // batch_size\n",
        "\n",
        "    act = np.zeros([inps.shape[0], 2048], dtype=np.float32)\n",
        "    for i in range(n_batches):\n",
        "        # Load a batch of data\n",
        "        inp = inps[i * batch_size:(i + 1) * batch_size]\n",
        "\n",
        "        # Resize each image to match the input shape of Inception v3\n",
        "        inpr = tf.image.resize(inp, (299, 299))\n",
        "\n",
        "        # Resize images in the interval [-1, 1], given that inpr is in [0, 1].\n",
        "        inpr = inpr * 2 - 1 \n",
        "\n",
        "        # Predict the activation\n",
        "        act[i * batch_size:(i + 1) * batch_size] = model.predict(inpr, steps=1)\n",
        "\n",
        "        print(f\"Processed {str((i + 1) * batch_size)} images.\")\n",
        "    return act\n",
        "\n",
        "\n",
        "def get_fid(images1, images2):\n",
        "    \"\"\"\n",
        "    Compute the FID between two sets of images.\n",
        "\n",
        "    Note: it can take several minutes.\n",
        "    \"\"\"\n",
        "    from scipy.linalg import sqrtm\n",
        "\n",
        "    shape = np.shape(images1)[1]\n",
        "    print(\"Computing FID for {} dimensional images\".format(images1.shape))\n",
        "\n",
        "    # Inception v3 requires the input to have 3 channel. If this is not the\n",
        "    # case, just copy the same channel three times.\n",
        "    if images1.shape[-1] == 1:\n",
        "        images1 = np.concatenate([images1, images1, images1], axis=-1)\n",
        "        images2 = np.concatenate([images2, images2, images2], axis=-1)\n",
        "\n",
        "    # activation for true images is always the same: we just compute it once\n",
        "    if os.path.exists(\"act_mu.npy\"):\n",
        "        mu1 = np.load(\"act_mu.npy\")\n",
        "        sigma1 = np.load(\"act_sigma.npy\")\n",
        "    else:\n",
        "        act1 = get_inception_activations(images1)\n",
        "        mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)\n",
        "        np.save(\"act_mu.npy\", mu1)\n",
        "        np.save(\"act_sigma.npy\", sigma1)\n",
        "    print('Done stage 1 of 2')\n",
        "\n",
        "    act2 = get_inception_activations(images2)\n",
        "    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)\n",
        "    print('Done stage 2 of 2')\n",
        "\n",
        "    # calculate sum squared difference between means\n",
        "    ssdiff = np.sum((mu1 - mu2) ** 2.0)\n",
        "\n",
        "    # compute sqrt of product between cov\n",
        "    covmean = sqrtm(sigma1.dot(sigma2))\n",
        "    # check and correct imaginary numbers from sqrt\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "\n",
        "    # calculate score\n",
        "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        "    return fid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OogQ0Wt3Taz7",
        "outputId": "9a8442b5-06db-45c7-99f8-60fcdb6874d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing FID for (10000, 32, 32, 1) dimensional images\n",
            "1/1 [==============================] - 23s 23s/step\n",
            "Processed 100 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 200 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 400 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 500 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 600 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 800 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 900 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 1000 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 1100 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 1200 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 1300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 1400 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 1500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 1600 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 1700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 1800 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 1900 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2000 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2100 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 2200 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2400 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2600 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 2700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2800 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 2900 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 3000 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 3100 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 3200 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 3300 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 3400 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 3500 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 3600 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 3700 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 3800 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 3900 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 4000 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 4100 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 4200 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 4300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 4400 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 4500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 4600 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 4700 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 4800 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 4900 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 5000 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 5100 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 5200 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 5300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 5400 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 5500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 5600 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 5700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 5800 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 5900 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 6000 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 6100 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 6200 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 6300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 6400 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 6500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 6600 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 6700 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 6800 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 6900 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 7000 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 7100 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 7200 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 7300 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 7400 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 7500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 7600 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 7700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 7800 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 7900 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 8000 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 8100 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 8200 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 8300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 8400 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 8500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 8600 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 8700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 8800 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 8900 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 9000 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 9100 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 9200 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 9300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 9400 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 9500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 9600 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 9700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 9800 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 9900 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 10000 images.\n",
            "Done stage 1 of 2\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 100 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 200 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 400 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 600 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 800 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 900 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 1000 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 1100 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 1200 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 1300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 1400 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 1500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 1600 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 1700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 1800 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 1900 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2000 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2100 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2200 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2300 images.\n",
            "1/1 [==============================] - 25s 25s/step\n",
            "Processed 2400 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2600 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 2700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2800 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 2900 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 3000 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 3100 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 3200 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 3300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 3400 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 3500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 3600 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 3700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 3800 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 3900 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 4000 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 4100 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 4200 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 4300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 4400 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 4500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 4600 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 4700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 4800 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 4900 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 5000 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 5100 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 5200 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 5300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 5400 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 5500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 5600 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 5700 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 5800 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 5900 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 6000 images.\n",
            "1/1 [==============================] - 23s 23s/step\n",
            "Processed 6100 images.\n",
            "1/1 [==============================] - 29s 29s/step\n",
            "Processed 6200 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 6300 images.\n",
            "1/1 [==============================] - 24s 24s/step\n",
            "Processed 6400 images.\n",
            "1/1 [==============================] - 27s 27s/step\n",
            "Processed 6500 images.\n",
            "1/1 [==============================] - 26s 26s/step\n",
            "Processed 6600 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 6700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 6800 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 6900 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 7000 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 7100 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 7200 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 7300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 7400 images.\n",
            "1/1 [==============================] - 25s 25s/step\n",
            "Processed 7500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 7600 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 7700 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 7800 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 7900 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 8000 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 8100 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 8200 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 8300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 8400 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 8500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 8600 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 8700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 8800 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 8900 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 9000 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 9100 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 9200 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 9300 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 9400 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 9500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 9600 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 9700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 9800 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 9900 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 10000 images.\n",
            "Done stage 2 of 2\n",
            "FID(x_test, x_train) = 2.6792390024659936\n"
          ]
        }
      ],
      "source": [
        "# Compute the FID between the Test set and (the first 10k images of) Train set (should be low)\n",
        "train_fid = get_fid(x_test, x_train[:10_000])\n",
        "\n",
        "# Print out the results\n",
        "print(f\"FID(x_test, x_train) = {train_fid}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onQ7QgToTaz9"
      },
      "source": [
        "# Good work!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!du -h *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8vay_urN-3Q",
        "outputId": "6f101e09-776c-4b29-e9c5-e773e7a0ef9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12K\tact_mu.npy\n",
            "33M\tact_sigma.npy\n",
            "55M\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras \n",
        "from keras.layers import BatchNormalization, Conv2D, Conv2DTranspose, Dense, Dropout, Flatten, Input, MaxPooling2D, ReLU, Rescaling, Reshape\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "import time\n",
        "import os"
      ],
      "metadata": {
        "id": "ggtSpoZyylbX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(x_train[3,:,:,0])\n",
        "plt.gray()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "AwOP1L-eno_O",
        "outputId": "bf171409-9397-491d-da2b-074919ea5113"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GeneratorDistribution(object):\n",
        "  # source: https://aylien.com/blog/introduction-generative-adversarial-networks-code-tensorflow\n",
        "  \n",
        "  def __init__(self, range):\n",
        "    self.range = range\n",
        "\n",
        "  def sample(self, N):\n",
        "    return np.linspace(-self.range, self.range, N) + np.random.random(N) * 0.01"
      ],
      "metadata": {
        "id": "yILRaKwyJSgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generator"
      ],
      "metadata": {
        "id": "Oe0GZKdHbpk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_generator_model(input_shape=(100,), output_shape=(32,32)):\n",
        "  \"\"\"\n",
        "  return the model of the generator\n",
        "  parameters:\n",
        "  output_shape is a tuple containing two values\n",
        "  \"\"\"\n",
        "  x = Input(shape=input_shape)\n",
        "\n",
        "  u1 = output_shape[0]//16\n",
        "  h1 = 512\n",
        "  dense = Dense(u1*u1*h1)(x)\n",
        "  norm = BatchNormalization()(dense)\n",
        "  layer = ReLU()(norm)\n",
        "  layer = Reshape((u1, u1, h1))(layer)\n",
        "\n",
        "  for i in range(4):\n",
        "    h1 = h1//2\n",
        "    strides = 1\n",
        "    for i in range(2):\n",
        "      layer = Conv2DTranspose(h1, 3, strides, padding='same')(layer)\n",
        "      layer = BatchNormalization()(layer)\n",
        "      layer = ReLU()(layer)\n",
        "      strides = strides * 2\n",
        "\n",
        "  # for i in range(3):\n",
        "  #   h1 = h1//2\n",
        "  #   u2 = u1*2\n",
        "  #   layer = Conv2DTranspose(h1, 3, 2, padding='same')(layer)\n",
        "\n",
        "  #   for i in range(2):\n",
        "  #     layer = Conv2D(h1, 3, 1, padding='same', activation='relu')(layer)\n",
        "\n",
        "  y = Conv2DTranspose(1,1,1, padding='same', activation='sigmoid')(layer)\n",
        "\n",
        "  model = Model(x, y)\n",
        "  print(model.summary())\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "bnBTklQnEiBJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = get_generator_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTCZST6j498C",
        "outputId": "7937d527-f71b-46d5-c1ba-4bc9a202b89a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 100)]             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2048)              206848    \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 2048)             8192      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " re_lu (ReLU)                (None, 2048)              0         \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 2, 2, 512)         0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 2, 2, 256)        1179904   \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 2, 2, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_1 (ReLU)              (None, 2, 2, 256)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 4, 4, 256)        590080    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 4, 4, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_2 (ReLU)              (None, 4, 4, 256)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 4, 4, 128)        295040    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 4, 4, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_3 (ReLU)              (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_3 (Conv2DT  (None, 8, 8, 128)        147584    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 8, 8, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_4 (ReLU)              (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_4 (Conv2DT  (None, 8, 8, 64)         73792     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 8, 8, 64)         256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_5 (ReLU)              (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_transpose_5 (Conv2DT  (None, 16, 16, 64)       36928     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 16, 16, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_6 (ReLU)              (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_6 (Conv2DT  (None, 16, 16, 32)       18464     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 16, 16, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_7 (ReLU)              (None, 16, 16, 32)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_7 (Conv2DT  (None, 32, 32, 32)       9248      \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 32, 32, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_8 (ReLU)              (None, 32, 32, 32)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_8 (Conv2DT  (None, 32, 32, 1)        33        \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,569,953\n",
            "Trainable params: 2,563,937\n",
            "Non-trainable params: 6,016\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# image generated without training\n",
        "noise = tf.random.normal([10, 100])\n",
        "generated_image = generator(noise, training=False)\n",
        "print(generated_image.shape)\n",
        "plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n",
        "plt.show()\n",
        "print(generated_image[0, :, 10, 0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        },
        "id": "FizgEja5oc8d",
        "outputId": "c38e81d2-a8dc-4a52-c0e7-5a78e37a5468"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 32, 32, 1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[0.50003445 0.50009626 0.5000688  0.50005203 0.5001103  0.5001222\n",
            " 0.5000418  0.50016975 0.5000751  0.50016016 0.500352   0.50001633\n",
            " 0.50025654 0.5001752  0.50015295 0.50018764 0.50036925 0.5001323\n",
            " 0.50027674 0.50004464 0.5005433  0.50015956 0.5002259  0.5002041\n",
            " 0.5002976  0.50012773 0.5002388  0.49996048 0.50016886 0.50020117\n",
            " 0.50025415 0.50001556], shape=(32,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discriminator"
      ],
      "metadata": {
        "id": "RZKXUpgkbvjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_discriminator_model(input_shape=(32,32,1)):\n",
        "  x = Input(shape=(input_shape))\n",
        "  # layer = Rescaling(1./255)(x)\n",
        "  # layer = Reshape((1,32,32))(x)\n",
        "  \n",
        "  d = input_shape[0]\n",
        "  h1 = 32\n",
        "  # layer = Conv2D(h1, 3, padding='same', activation='relu', data_format='channels_first')(layer)\n",
        "  layer = Conv2D(h1, 3, padding='same', activation='relu')(x)\n",
        "  \n",
        "  for i in range(4):\n",
        "    if i != 0:\n",
        "      layer = Conv2D(h1, 3, padding='same', activation='relu')(layer)\n",
        "    layer = Conv2D(h1, 3, padding='same', activation='relu')(layer)\n",
        "    \n",
        "    #layer = Reshape((d,d,h1))(layer)\n",
        "    layer = MaxPooling2D((2,2))(layer)\n",
        "    h1 = h1*2\n",
        "    d = d//2\n",
        "  \n",
        "  layer = Dropout(.2)(layer)\n",
        "  layer = Flatten()(layer)\n",
        "  layer = Dense(h1//4, activation='relu')(layer)\n",
        "  # layer = Dense(h1//8, activation='relu')(layer)\n",
        "  \n",
        "  \n",
        "  y = Dense(1, activation='sigmoid')(layer)\n",
        "\n",
        "  model = Model(x, y)\n",
        "  print(model.summary())\n",
        "  return model"
      ],
      "metadata": {
        "id": "4X5pMsEJ_y6C"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = get_discriminator_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J33ghwEAgzCA",
        "outputId": "f23274bb-e6bc-4321-98b0-40780c808309"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_7 (InputLayer)        [(None, 32, 32, 1)]       0         \n",
            "                                                                 \n",
            " conv2d_34 (Conv2D)          (None, 32, 32, 32)        320       \n",
            "                                                                 \n",
            " conv2d_35 (Conv2D)          (None, 32, 32, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPooling  (None, 16, 16, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_36 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " conv2d_37 (Conv2D)          (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPoolin  (None, 8, 8, 64)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_38 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " conv2d_39 (Conv2D)          (None, 8, 8, 128)         147584    \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPoolin  (None, 4, 4, 128)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_40 (Conv2D)          (None, 4, 4, 256)         295168    \n",
            "                                                                 \n",
            " conv2d_41 (Conv2D)          (None, 4, 4, 256)         590080    \n",
            "                                                                 \n",
            " max_pooling2d_12 (MaxPoolin  (None, 2, 2, 256)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 2, 2, 256)         0         \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 128)               131200    \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,303,009\n",
            "Trainable params: 1,303,009\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss function and optimizers"
      ],
      "metadata": {
        "id": "hZ1U4Fz7kPL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir training_checkpoints"
      ],
      "metadata": {
        "id": "aGQQTr-LkHbM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n"
      ],
      "metadata": {
        "id": "cn5ptbkWjy_5"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n"
      ],
      "metadata": {
        "id": "_8NF1mXLkLlE"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "LwnRSdwdkc1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "EPOCHS = 50\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 16\n",
        "\n",
        "# Batch and shuffle the data\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "print(f\"{len(train_dataset)}\")\n",
        "\n",
        "# You will reuse this seed overtime (so it's easier)\n",
        "# to visualize progress in the animated GIF)\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n"
      ],
      "metadata": {
        "id": "4wR28yNLkgFI",
        "outputId": "dbcca137-6de2-4160-fb8d-b34f6c207e7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "noise = None\n",
        "generated_images = None\n",
        "real_output = None\n",
        "fake_output = None\n",
        "gen_loss = None\n",
        "disc_loss = None\n",
        "gradients_of_generator = None\n",
        "gradients_of_discriminator = None\n",
        "\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "    # print(f\"images shape: {images.shape}\\nnoise shape: {noise.shape}\")\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "      # print(f\"generated images shape: {generated_images.shape}\")\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n"
      ],
      "metadata": {
        "id": "ebkSSrMAlAij"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      # print(f\"image batch shape: {image_batch.shape}\")\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # Produce images for the GIF as you go\n",
        "    # display.clear_output(wait=True)\n",
        "    # generate_and_save_images(generator,epoch + 1,seed)\n",
        "\n",
        "    # Save the model every 15 epochs\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # Generate after the final epoch\n",
        "  # display.clear_output(wait=True)\n",
        "  # generate_and_save_images(generator,epochs,seed)\n",
        "\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "lr9ex2qZlEMx"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_dataset, EPOCHS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTBxIjfemc7M",
        "outputId": "95c4b9ce-2148-4590-a82f-e0035455ce06"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for epoch 1 is 36.66096520423889 sec\n",
            "Time for epoch 2 is 24.15246343612671 sec\n",
            "Time for epoch 3 is 23.783912897109985 sec\n",
            "Time for epoch 4 is 23.815901517868042 sec\n",
            "Time for epoch 5 is 24.011834859848022 sec\n",
            "Time for epoch 6 is 23.90442991256714 sec\n",
            "Time for epoch 7 is 23.780311584472656 sec\n",
            "Time for epoch 8 is 23.77782678604126 sec\n",
            "Time for epoch 9 is 23.821516036987305 sec\n",
            "Time for epoch 10 is 23.885895013809204 sec\n",
            "Time for epoch 11 is 23.76420497894287 sec\n",
            "Time for epoch 12 is 23.763095140457153 sec\n",
            "Time for epoch 13 is 23.831923484802246 sec\n",
            "Time for epoch 14 is 23.866856813430786 sec\n",
            "Time for epoch 15 is 24.357905626296997 sec\n",
            "Time for epoch 16 is 23.796587467193604 sec\n",
            "Time for epoch 17 is 23.749140739440918 sec\n",
            "Time for epoch 18 is 23.765610456466675 sec\n",
            "Time for epoch 19 is 23.743924379348755 sec\n",
            "Time for epoch 20 is 23.70754361152649 sec\n",
            "Time for epoch 21 is 23.775134801864624 sec\n",
            "Time for epoch 22 is 23.720370292663574 sec\n",
            "Time for epoch 23 is 23.755584239959717 sec\n",
            "Time for epoch 24 is 23.780417919158936 sec\n",
            "Time for epoch 25 is 23.76087498664856 sec\n",
            "Time for epoch 26 is 23.77956485748291 sec\n",
            "Time for epoch 27 is 23.84302067756653 sec\n",
            "Time for epoch 28 is 23.802905321121216 sec\n",
            "Time for epoch 29 is 23.726680278778076 sec\n",
            "Time for epoch 30 is 24.09148645401001 sec\n",
            "Time for epoch 31 is 23.70971155166626 sec\n",
            "Time for epoch 32 is 23.720887184143066 sec\n",
            "Time for epoch 33 is 23.825047254562378 sec\n",
            "Time for epoch 34 is 23.735572814941406 sec\n",
            "Time for epoch 35 is 23.774911403656006 sec\n",
            "Time for epoch 36 is 23.753085613250732 sec\n",
            "Time for epoch 37 is 23.745118618011475 sec\n",
            "Time for epoch 38 is 23.742589473724365 sec\n",
            "Time for epoch 39 is 23.743446111679077 sec\n",
            "Time for epoch 40 is 23.742764949798584 sec\n",
            "Time for epoch 41 is 23.742505073547363 sec\n",
            "Time for epoch 42 is 23.730693578720093 sec\n",
            "Time for epoch 43 is 23.731065034866333 sec\n",
            "Time for epoch 44 is 23.74129366874695 sec\n",
            "Time for epoch 45 is 24.048184394836426 sec\n",
            "Time for epoch 46 is 23.689396381378174 sec\n",
            "Time for epoch 47 is 23.750577926635742 sec\n",
            "Time for epoch 48 is 23.733288049697876 sec\n",
            "Time for epoch 49 is 23.74473810195923 sec\n",
            "Time for epoch 50 is 23.746391534805298 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "metadata": {
        "id": "2biPUQ_hy1FN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.11 ('ml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "2db16da740494d35c7f3749582ea0fec8725a5f0d9e976cf583dd1041e9a5f0f"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}