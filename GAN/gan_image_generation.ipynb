{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSWSrJ-iTazX"
      },
      "source": [
        "# Generating Images with Generative Adversarial Networks (GANs)\n",
        "\n",
        "The purpose of the project is to test the ability of Generative Adversial Networks (GANs) in generating realistic-looking images. \n",
        "\n",
        "## Dataset\n",
        "\n",
        "The dataset used will be FashionMNIST. It contains low resolution ($28 \\times 28$) grey-scale images representing different kind of clothes. The dataset is available on keras and accessable in $\\texttt{tf.keras.datasets.fashion\\_mnist}$. Note that the pixel values for the images are initially in the interval $[0, 255]$. It is required to normalize them since all of the algorithm we will use require them to be in that format. To be fair, you will find the dataset already normalized, do not modify that part of the code.\n",
        "\n",
        "## Metrics\n",
        "\n",
        "Measuring the quality of newly generated images is a non-trivial task. Indeed, there is no label associated to each image, and thus it is impossible to measure the quality image-by-image. For that reason, common metrics uses statistical consideration on a generated dataset to test how well the network recovered the statistics of the original data. One of the most common is the Fr√©chet Inception Distance (FID). The idea of FID is that in a realistic-looking dataset of images, the statistics of the activation of the last hidden layer in a well-trained classificator should be similar to that of a dataset containing real images. Specifically, regarding FID, the Inception-v3 network is used as a classificator. A real dataset $\\mathbb{D}_r$ and a generated dataset $\\mathbb{D}_g$ are processed by the network, and the activation of the last hidden layer has mean and variance $(\\mu_r, \\Sigma_r)$, $(\\mu_g, \\Sigma_g)$ respectively. Then, FID is computed as:\n",
        "\n",
        "$$\n",
        "    FID(\\mathbb{D}_r, \\mathbb{D}_g) = || \\mu_r - \\mu_g ||^2 + Tr(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\ast \\Sigma_g)^{\\frac{1}{2}}) \n",
        "$$\n",
        "\n",
        "A Python implementation of FID can be found in the file $\\texttt{fid.py}$ that you find attached on Virtuale. Its usage is very simple, just generate $10k$ fake images with your GAN, and with the command $\\texttt{fid.get\\_fid(x\\_test, x\\_gen)}$, where $\\texttt{x\\_test}$ is the test set, containing $10k$ real images, you get the value for the FID of your network. Remember that, when passed through that function, $\\texttt{x\\_gen}$ **must** be a dataset of $10k$ images, in the interval $[0, 1]$. The number of $10k$ images is fundamental, since the value of FID strongly depends on the number of input images.\n",
        "\n",
        "## Limitations\n",
        "\n",
        "You are required to implement a vanilla Generative Adversarial Network (GAN), not a variant of it (e.g. PixelGAN, CycleGAN, ... are **not** accepted). The maximum number of parameters is *15 million*, and every pre-trained network can be used as an add-on (the number of parameters for pre-trained network does not count). Clearly, only the training set can be used to train the network, no additional images (Data Augmentation is ok)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aaubgTXbTazn"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceos7zgTTazs"
      },
      "source": [
        "The images are normalized in $[0, 1]$. For simplicity, images are padded to have dimension $32 \\times 32$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Es5LDnq0Tazv",
        "outputId": "b4fc91e8-0798-4831-e840-ce1bd58b3b57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 1us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 1s 0us/step\n",
            "Training shape: (60000, 32, 32, 1), Training pixel values: (0.0, 1.0)\n",
            "Test shape: (10000, 32, 32, 1), Test pixel values: (0.0, 1.0)\n"
          ]
        }
      ],
      "source": [
        "# Load the data. Note that the labels y_train and y_test are not loaded since not required.\n",
        "(x_train, _), (x_test, _) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize and pad the datasets\n",
        "x_train = np.pad(x_train, ((0,0), (2,2), (2,2)))\n",
        "x_train = np.reshape(x_train, x_train.shape + (1, ))\n",
        "x_train = x_train / 255.\n",
        "\n",
        "x_test = np.pad(x_test, ((0,0), (2,2), (2,2)))\n",
        "x_test = np.reshape(x_test, x_test.shape + (1, ))\n",
        "x_test = x_test / 255.\n",
        "\n",
        "print(f\"Training shape: {x_train.shape}, Training pixel values: {x_train.min(), x_train.max()}\")\n",
        "print(f\"Test shape: {x_test.shape}, Test pixel values: {x_test.min(), x_test.max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRRpm4TkTazy"
      },
      "source": [
        "Now, we import the functions for the computation of the FID, and we test that FID(x_train, x_test) is low.\n",
        "\n",
        "_Note: Computing the FID function requires some minutes. Consequently, it is suggested to comment this cell after you tested once, to reduce the execution time of the notebook. To speed-up the process, after a first use, the function will generate a file containing the value of the activations of the test set, so that it does not have to compute it again every time._ \n",
        "\n",
        "**Remember that, when you use the FID function, the first input MUST be the test set, while the second will be the generated images set.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dPZU1oiTaz1",
        "outputId": "a409e65f-c760-4475-bb6f-cdf2155a74be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 4s 0us/step\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Do not modify this code. This is just for utilities.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "# prepare the inception v3 model\n",
        "model = InceptionV3(include_top=False, pooling='avg', input_shape=(299, 299, 3), weights='imagenet')\n",
        "\n",
        "def get_inception_activations(inps, batch_size=100):\n",
        "    \"\"\"\n",
        "    Compute the activation for the model Inception v3 for a given input 'inps'.\n",
        "\n",
        "    Note: inps is assumed to be normalized in [0, 1].\n",
        "    \"\"\"\n",
        "    n_batches = inps.shape[0] // batch_size\n",
        "\n",
        "    act = np.zeros([inps.shape[0], 2048], dtype=np.float32)\n",
        "    for i in range(n_batches):\n",
        "        # Load a batch of data\n",
        "        inp = inps[i * batch_size:(i + 1) * batch_size]\n",
        "\n",
        "        # Resize each image to match the input shape of Inception v3\n",
        "        inpr = tf.image.resize(inp, (299, 299))\n",
        "\n",
        "        # Resize images in the interval [-1, 1], given that inpr is in [0, 1].\n",
        "        inpr = inpr * 2 - 1 \n",
        "\n",
        "        # Predict the activation\n",
        "        act[i * batch_size:(i + 1) * batch_size] = model.predict(inpr, steps=1)\n",
        "\n",
        "        print(f\"Processed {str((i + 1) * batch_size)} images.\")\n",
        "    return act\n",
        "\n",
        "\n",
        "def get_fid(images1, images2):\n",
        "    \"\"\"\n",
        "    Compute the FID between two sets of images.\n",
        "\n",
        "    Note: it can take several minutes.\n",
        "    \"\"\"\n",
        "    from scipy.linalg import sqrtm\n",
        "\n",
        "    shape = np.shape(images1)[1]\n",
        "    print(\"Computing FID for {} dimensional images\".format(images1.shape))\n",
        "\n",
        "    # Inception v3 requires the input to have 3 channel. If this is not the\n",
        "    # case, just copy the same channel three times.\n",
        "    if images1.shape[-1] == 1:\n",
        "        images1 = np.concatenate([images1, images1, images1], axis=-1)\n",
        "        images2 = np.concatenate([images2, images2, images2], axis=-1)\n",
        "\n",
        "    # activation for true images is always the same: we just compute it once\n",
        "    if os.path.exists(\"act_mu.npy\"):\n",
        "        mu1 = np.load(\"act_mu.npy\")\n",
        "        sigma1 = np.load(\"act_sigma.npy\")\n",
        "    else:\n",
        "        act1 = get_inception_activations(images1)\n",
        "        mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)\n",
        "        np.save(\"act_mu.npy\", mu1)\n",
        "        np.save(\"act_sigma.npy\", sigma1)\n",
        "    print('Done stage 1 of 2')\n",
        "\n",
        "    act2 = get_inception_activations(images2)\n",
        "    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)\n",
        "    print('Done stage 2 of 2')\n",
        "\n",
        "    # calculate sum squared difference between means\n",
        "    ssdiff = np.sum((mu1 - mu2) ** 2.0)\n",
        "\n",
        "    # compute sqrt of product between cov\n",
        "    covmean = sqrtm(sigma1.dot(sigma2))\n",
        "    # check and correct imaginary numbers from sqrt\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "\n",
        "    # calculate score\n",
        "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        "    return fid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OogQ0Wt3Taz7",
        "outputId": "9a8442b5-06db-45c7-99f8-60fcdb6874d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing FID for (10000, 32, 32, 1) dimensional images\n",
            "1/1 [==============================] - 23s 23s/step\n",
            "Processed 100 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 200 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 400 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 500 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 600 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 800 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 900 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 1000 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 1100 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 1200 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 1300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 1400 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 1500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 1600 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 1700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 1800 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 1900 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2000 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2100 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 2200 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2400 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2600 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 2700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2800 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 2900 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 3000 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 3100 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 3200 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 3300 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 3400 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 3500 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 3600 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 3700 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 3800 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 3900 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 4000 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 4100 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 4200 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 4300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 4400 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 4500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 4600 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 4700 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 4800 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 4900 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 5000 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 5100 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 5200 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 5300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 5400 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 5500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 5600 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 5700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 5800 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 5900 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 6000 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 6100 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 6200 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 6300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 6400 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 6500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 6600 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 6700 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 6800 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 6900 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 7000 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 7100 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 7200 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 7300 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 7400 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 7500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 7600 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 7700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 7800 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 7900 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 8000 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 8100 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 8200 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 8300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 8400 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 8500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 8600 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 8700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 8800 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 8900 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 9000 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 9100 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 9200 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 9300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 9400 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 9500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 9600 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 9700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 9800 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 9900 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 10000 images.\n",
            "Done stage 1 of 2\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 100 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 200 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 400 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 600 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 800 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 900 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 1000 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 1100 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 1200 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 1300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 1400 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 1500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 1600 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 1700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 1800 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 1900 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2000 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2100 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2200 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2300 images.\n",
            "1/1 [==============================] - 25s 25s/step\n",
            "Processed 2400 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2600 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 2700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 2800 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 2900 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 3000 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 3100 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 3200 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 3300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 3400 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 3500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 3600 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 3700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 3800 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 3900 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 4000 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 4100 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 4200 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 4300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 4400 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 4500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 4600 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 4700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 4800 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 4900 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 5000 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 5100 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 5200 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 5300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 5400 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 5500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 5600 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 5700 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 5800 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 5900 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 6000 images.\n",
            "1/1 [==============================] - 23s 23s/step\n",
            "Processed 6100 images.\n",
            "1/1 [==============================] - 29s 29s/step\n",
            "Processed 6200 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 6300 images.\n",
            "1/1 [==============================] - 24s 24s/step\n",
            "Processed 6400 images.\n",
            "1/1 [==============================] - 27s 27s/step\n",
            "Processed 6500 images.\n",
            "1/1 [==============================] - 26s 26s/step\n",
            "Processed 6600 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 6700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 6800 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 6900 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 7000 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 7100 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 7200 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 7300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 7400 images.\n",
            "1/1 [==============================] - 25s 25s/step\n",
            "Processed 7500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 7600 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 7700 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 7800 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 7900 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 8000 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 8100 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 8200 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 8300 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 8400 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 8500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 8600 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 8700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 8800 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 8900 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 9000 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 9100 images.\n",
            "1/1 [==============================] - 20s 20s/step\n",
            "Processed 9200 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 9300 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 9400 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 9500 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 9600 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 9700 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 9800 images.\n",
            "1/1 [==============================] - 21s 21s/step\n",
            "Processed 9900 images.\n",
            "1/1 [==============================] - 22s 22s/step\n",
            "Processed 10000 images.\n",
            "Done stage 2 of 2\n",
            "FID(x_test, x_train) = 2.6792390024659936\n"
          ]
        }
      ],
      "source": [
        "# Compute the FID between the Test set and (the first 10k images of) Train set (should be low)\n",
        "train_fid = get_fid(x_test, x_train[:10_000])\n",
        "\n",
        "# Print out the results\n",
        "print(f\"FID(x_test, x_train) = {train_fid}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onQ7QgToTaz9"
      },
      "source": [
        "# Good work!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!du -h *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8vay_urN-3Q",
        "outputId": "6f101e09-776c-4b29-e9c5-e773e7a0ef9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12K\tact_mu.npy\n",
            "33M\tact_sigma.npy\n",
            "55M\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras \n",
        "from keras.layers import BatchNormalization, Conv2D, Conv2DTranspose, Dense, Dropout, Flatten, Input, MaxPooling2D, ReLU, Rescaling, Reshape\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "import time\n",
        "import os"
      ],
      "metadata": {
        "id": "ggtSpoZyylbX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(x_train[3,:,:,0])\n",
        "plt.show()\n",
        "print(x_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "AwOP1L-eno_O",
        "outputId": "47901621-a25b-46d9-a21c-760f3d500756"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 32, 32, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GeneratorDistribution(object):\n",
        "  # source: https://aylien.com/blog/introduction-generative-adversarial-networks-code-tensorflow\n",
        "  \n",
        "  def __init__(self, range):\n",
        "    self.range = range\n",
        "\n",
        "  def sample(self, N):\n",
        "    return np.linspace(-self.range, self.range, N) + np.random.random(N) * 0.01"
      ],
      "metadata": {
        "id": "yILRaKwyJSgO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generator"
      ],
      "metadata": {
        "id": "Oe0GZKdHbpk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_generator_model(input_shape=(100,), output_shape=(32,32)):\n",
        "  \"\"\"\n",
        "  return the model of the generator\n",
        "  parameters:\n",
        "  output_shape is a tuple containing two values\n",
        "  \"\"\"\n",
        "  x = Input(shape=input_shape)\n",
        "\n",
        "  u1 = output_shape[0]//16\n",
        "  h1 = 512\n",
        "  dense = Dense(u1*u1*h1)(x)\n",
        "  norm = BatchNormalization()(dense)\n",
        "  layer = ReLU()(norm)\n",
        "  layer = Reshape((u1, u1, h1))(layer)\n",
        "\n",
        "  for i in range(4):\n",
        "    h1 = h1//2\n",
        "    strides = 1\n",
        "    for i in range(2):\n",
        "      layer = Conv2DTranspose(h1, 3, strides, padding='same')(layer)\n",
        "      layer = BatchNormalization()(layer)\n",
        "      layer = ReLU()(layer)\n",
        "      strides = strides * 2\n",
        "\n",
        "  # for i in range(3):\n",
        "  #   h1 = h1//2\n",
        "  #   u2 = u1*2\n",
        "  #   layer = Conv2DTranspose(h1, 3, 2, padding='same')(layer)\n",
        "\n",
        "  #   for i in range(2):\n",
        "  #     layer = Conv2D(h1, 3, 1, padding='same', activation='relu')(layer)\n",
        "\n",
        "  y = Conv2DTranspose(1,1,1, padding='same', activation='sigmoid')(layer)\n",
        "\n",
        "  model = Model(x, y)\n",
        "  print(model.summary())\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "bnBTklQnEiBJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = get_generator_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTCZST6j498C",
        "outputId": "32cf0396-a442-4b8b-fa2a-acabecba43e8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 100)]             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2048)              206848    \n",
            "                                                                 \n",
            " batch_normalization_94 (Bat  (None, 2048)             8192      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu (ReLU)                (None, 2048)              0         \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 2, 2, 512)         0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 2, 2, 256)        1179904   \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " batch_normalization_95 (Bat  (None, 2, 2, 256)        1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_1 (ReLU)              (None, 2, 2, 256)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 4, 4, 256)        590080    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_96 (Bat  (None, 4, 4, 256)        1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_2 (ReLU)              (None, 4, 4, 256)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 4, 4, 128)        295040    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_97 (Bat  (None, 4, 4, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_3 (ReLU)              (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_3 (Conv2DT  (None, 8, 8, 128)        147584    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_98 (Bat  (None, 8, 8, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_4 (ReLU)              (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_4 (Conv2DT  (None, 8, 8, 64)         73792     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_99 (Bat  (None, 8, 8, 64)         256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_5 (ReLU)              (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_transpose_5 (Conv2DT  (None, 16, 16, 64)       36928     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_100 (Ba  (None, 16, 16, 64)       256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " re_lu_6 (ReLU)              (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_6 (Conv2DT  (None, 16, 16, 32)       18464     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_101 (Ba  (None, 16, 16, 32)       128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " re_lu_7 (ReLU)              (None, 16, 16, 32)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_7 (Conv2DT  (None, 32, 32, 32)       9248      \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_102 (Ba  (None, 32, 32, 32)       128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " re_lu_8 (ReLU)              (None, 32, 32, 32)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_8 (Conv2DT  (None, 32, 32, 1)        33        \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,569,953\n",
            "Trainable params: 2,563,937\n",
            "Non-trainable params: 6,016\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# image generated without training\n",
        "noise = tf.random.normal([1, 100])\n",
        "generated_image = generator(noise, training=False)\n",
        "print(generated_image.shape)\n",
        "plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n",
        "plt.show()\n",
        "print(generated_image[0, :, 10, 0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        },
        "id": "FizgEja5oc8d",
        "outputId": "c76e5e42-d0ef-4c3f-8f16-415dc389b2ab"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 32, 32, 1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[0.4999746  0.49997425 0.50011146 0.49994218 0.4998589  0.5000931\n",
            " 0.50008667 0.5000515  0.5001246  0.4999264  0.5006915  0.49994636\n",
            " 0.50017554 0.49998337 0.5002081  0.5003339  0.49993348 0.49997008\n",
            " 0.50009495 0.4998885  0.5000013  0.5000773  0.50030285 0.4999693\n",
            " 0.50001    0.49979374 0.5001611  0.49986365 0.5000804  0.49985346\n",
            " 0.50016975 0.5000145 ], shape=(32,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discriminator"
      ],
      "metadata": {
        "id": "RZKXUpgkbvjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_discriminator_model(input_shape=(32,32,1)):\n",
        "  x = Input(shape=(input_shape))\n",
        "  # layer = Rescaling(1./255)(x)\n",
        "  # layer = Reshape((1,32,32))(x)\n",
        "  \n",
        "  d = input_shape[0]\n",
        "  h1 = 32\n",
        "  # layer = Conv2D(h1, 3, padding='same', activation='relu', data_format='channels_first')(layer)\n",
        "  layer = Conv2D(h1, 3, padding='same', activation='relu')(x)\n",
        "  \n",
        "  for i in range(4):\n",
        "    if i != 0:\n",
        "      layer = Conv2D(h1, 3, padding='same', activation='relu')(layer)\n",
        "    layer = Conv2D(h1, 3, padding='same', strides=(2, 2), activation='relu')(layer)\n",
        "    \n",
        "    #layer = Reshape((d,d,h1))(layer)\n",
        "    # layer = MaxPooling2D((2,2), padding='same')(layer)\n",
        "    h1 = h1*2\n",
        "    d = d//2\n",
        "  \n",
        "  layer = Dropout(.2)(layer)\n",
        "  layer = Flatten()(layer)\n",
        "  layer = Dense(h1//4, activation='relu')(layer)\n",
        "  # layer = Dense(h1//8, activation='relu')(layer)\n",
        "  \n",
        "  y = Dense(1, activation='sigmoid')(layer)\n",
        "\n",
        "  model = Model(x, y)\n",
        "  print(model.summary())\n",
        "  return model"
      ],
      "metadata": {
        "id": "4X5pMsEJ_y6C"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = get_discriminator_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J33ghwEAgzCA",
        "outputId": "8d44267f-ca72-45e2-9ee8-7fbbe5de5b6e"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_27\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_40 (InputLayer)       [(None, 32, 32, 1)]       0         \n",
            "                                                                 \n",
            " conv2d_322 (Conv2D)         (None, 32, 32, 32)        320       \n",
            "                                                                 \n",
            " conv2d_323 (Conv2D)         (None, 16, 16, 32)        9248      \n",
            "                                                                 \n",
            " conv2d_324 (Conv2D)         (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " conv2d_325 (Conv2D)         (None, 8, 8, 64)          36928     \n",
            "                                                                 \n",
            " conv2d_326 (Conv2D)         (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " conv2d_327 (Conv2D)         (None, 4, 4, 128)         147584    \n",
            "                                                                 \n",
            " conv2d_328 (Conv2D)         (None, 4, 4, 256)         295168    \n",
            "                                                                 \n",
            " conv2d_329 (Conv2D)         (None, 2, 2, 256)         590080    \n",
            "                                                                 \n",
            " dropout_26 (Dropout)        (None, 2, 2, 256)         0         \n",
            "                                                                 \n",
            " flatten_26 (Flatten)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_53 (Dense)            (None, 128)               131200    \n",
            "                                                                 \n",
            " dense_54 (Dense)            (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,303,009\n",
            "Trainable params: 1,303,009\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss function and optimizers"
      ],
      "metadata": {
        "id": "hZ1U4Fz7kPL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir training_checkpoints"
      ],
      "metadata": {
        "id": "aGQQTr-LkHbM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n"
      ],
      "metadata": {
        "id": "cn5ptbkWjy_5"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n"
      ],
      "metadata": {
        "id": "_8NF1mXLkLlE"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "LwnRSdwdkc1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 16\n",
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "# Batch and shuffle the data\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "# You will reuse this seed overtime (so it's easier)\n",
        "# to visualize progress in the animated GIF)\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n"
      ],
      "metadata": {
        "id": "4wR28yNLkgFI"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n"
      ],
      "metadata": {
        "id": "ebkSSrMAlAij"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # Produce images for the GIF as you go\n",
        "    # display.clear_output(wait=True)\n",
        "    # generate_and_save_images(generator,epoch + 1,seed)\n",
        "\n",
        "    # Save the model every 15 epochs\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # Generate after the final epoch\n",
        "  # display.clear_output(wait=True)\n",
        "  # generate_and_save_images(generator,epochs,seed)\n",
        "\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "lr9ex2qZlEMx"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(x_train, EPOCHS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "jTBxIjfemc7M",
        "outputId": "96b4c650-88ec-4d4f-ef4c-d4bd1366ff30"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-152-f306a93d5dc3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-148-45d2e8f8f03e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m       \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Produce images for the GIF as you go\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_file0rc97ild.py\u001b[0m in \u001b[0;36mtf__train_step\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgen_tape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdisc_tape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0mgenerated_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                     \u001b[0mreal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                     \u001b[0mfake_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mgen_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    278\u001b[0m                     \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 }:\n\u001b[0;32m--> 280\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    281\u001b[0m                         \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                         \u001b[0;34mf\"incompatible with the layer: expected axis {axis} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"<ipython-input-93-ae02327b3832>\", line 10, in train_step  *\n        real_output = discriminator(images, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/input_spec.py\", line 280, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'model_27' (type Functional).\n    \n    Input 0 of layer \"dense_53\" is incompatible with the layer: expected axis -1 of input shape to have value 1024, but received input with shape (32, 512)\n    \n    Call arguments received by layer 'model_27' (type Functional):\n      ‚Ä¢ inputs=tf.Tensor(shape=(32, 32, 1), dtype=float64)\n      ‚Ä¢ training=True\n      ‚Ä¢ mask=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "metadata": {
        "id": "2biPUQ_hy1FN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.11 ('ml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "2db16da740494d35c7f3749582ea0fec8725a5f0d9e976cf583dd1041e9a5f0f"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}