{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of the project is to learn the mapping from polar coordinates to a a discrete 10x10 grid of cells in the plane, using a neural network. \n",
        "\n",
        "The supervised dataset is given to you in the form of a generator (to be considered as a black box).\n",
        "\n",
        "The model must achieve an accuracy of 95%, and it will be evaluated in a way **inversely proportional to the number of its parameters: the smaller, the better.**\n",
        "\n",
        "**WARNING**: Any solution taking advantage of meta-knowledge about the generator will be automatically rejected."
      ],
      "metadata": {
        "id": "Zw_326KLT9dF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ynz-4_4cFmbJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, concatenate\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the generator. It returns triples of the form ((theta,rho),out) where (theta,rho) are the polar coordinates of a point in the first quadrant of the plane, and out is a 10x10 map with \"1\" in the cell corresponding to the point position, and \"0\" everywhere else.\n",
        "\n",
        "By setting flat=True, the resulting map is flattened into a vector with a single dimension 100. You can use this variant, if you wish. "
      ],
      "metadata": {
        "id": "iA01pkKbUt7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def polar_generator(batchsize,grid=(10,10),noise=.002,flat=False):\n",
        "  while True:\n",
        "    x = np.random.rand(batchsize)\n",
        "    y = np.random.rand(batchsize)\n",
        "    out = np.zeros((batchsize,grid[0],grid[1]))\n",
        "    xc = (x*grid[0]).astype(int)\n",
        "    yc = (y*grid[1]).astype(int)\n",
        "    for b in range(batchsize):\n",
        "      out[b,xc[b],yc[b]] = 1\n",
        "    #compute rho and theta and add some noise\n",
        "    rho = np.sqrt(x**2+y**2) + np.random.normal(scale=noise)\n",
        "    theta = np.arctan(y/np.maximum(x,.00001)) + np.random.normal(scale=noise)\n",
        "    if flat:\n",
        "      out = np.reshape(out,(batchsize,grid[0]*grid[1]))\n",
        "    yield ((theta,rho),out)"
      ],
      "metadata": {
        "id": "DsA1GqAeWAdo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create an instance of the generator on a grid with dimension 3x4"
      ],
      "metadata": {
        "id": "ZF-jlaqAWc2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g1,g2 = 3,4\n",
        "gen = polar_generator(1,grid=(g1,g2),noise=0.0)"
      ],
      "metadata": {
        "id": "Ov3rXaLVHDCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now let's see a few samples."
      ],
      "metadata": {
        "id": "b4hntQtSWjPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(theta,rho),maps = next(gen)\n",
        "for i,map in enumerate(maps):\n",
        "  #let us compute the cartesian coordinates\n",
        "  x = np.cos(theta[i])*rho[i]\n",
        "  y = np.sin(theta[i])*rho[i]\n",
        "  print(\"x coordinate (row): {}\".format(int(x*g1)))\n",
        "  print(\"y coordinate (col): {}\".format(int(y*g2)))\n",
        "  print(\"map:\")\n",
        "  print(np.reshape(map,(g1,g2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PM7R8ZZZHN7p",
        "outputId": "92c37cd5-6f7e-4d40-d57c-c4a132a739d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x coordinate (row): 0\n",
            "y coordinate (col): 1\n",
            "map:\n",
            "[[0. 1. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise: add noise to the generator, and check the effect on the \"ground truth\"."
      ],
      "metadata": {
        "id": "NTY5fu8Hg7RE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What to deliver\n",
        "\n",
        "For the purposes of the project you are supposed to work with the **default 10x10 grid, and the default noise=.002**\n",
        "\n",
        "The generator must be treatead as a black box, do not tweak it, and do not exploit its semantics that is supposed to be unknown. You are allowed to work with the \"flat\" modality, if you prefer so.\n",
        "\n",
        "You need to:\n",
        "1.   define an accuracy function (take inspiration from the code of the previous cell)\n",
        "2.   define a neural network taking in input theta and rho, and returning out\n",
        "3. measure the network's accuracy that must be above 95% (accuracy must be evaluated over at least 20000 samples)\n",
        "4. tune the network trying to decrease as much as possible the numer of parameters, preserving an accuracy above 95%. Only your best network must be delivered.\n",
        "\n",
        "You must deliver a SINGLE notebook working on colab, containing the code of the network, its summary, the training history, the code for the accurary metric and its evaluation on the network.\n",
        "\n",
        "**N.B.** The accuracy must be above 95% but apart from that it does not influence the evaluation. You score will only depend on the number of parameters: the lower, the better.\n",
        "\n",
        "#Good work!\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jj4akvA24maJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.experimental import AdamW\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import LeakyReLU, Dropout"
      ],
      "metadata": {
        "id": "_pjE0IGoyVW-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_unit(n: int):\n",
        "  res = 1\n",
        "  while(2^res < n):\n",
        "    res += 1\n",
        "  return res"
      ],
      "metadata": {
        "id": "0oiK7MDZpVaq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_network(output_shape=(10,10)):\n",
        "  #theta, ro = theta_ro\n",
        "  out1, out2 = output_shape\n",
        "  out_area = out1 * out2\n",
        "\n",
        "  input1 = Input(shape=(1,))\n",
        "  input2 = Input(shape=(1,))\n",
        "  merged = concatenate([input1, input2])\n",
        "\n",
        "  d1 = Dense(64)(merged)\n",
        "  d1 = LeakyReLU()(d1)\n",
        "  d1 = Dense(64)(d1)\n",
        "  d1 = LeakyReLU()(d1)\n",
        "  d1 = Dropout(0.2)(d1)\n",
        "  d2 = Dense(out_area, activation='softmax')(d1)\n",
        "  \n",
        "  model = Model(inputs=[input1, input2], outputs=d2)\n",
        "  print(model.summary())\n",
        "  return model\n",
        "\n",
        "# def get_network(output_shape=(10,10)):\n",
        "#   out1, out2 = output_shape\n",
        "#   out_area = out1 * out2\n",
        "\n",
        "#   input1 = Input(shape=(1,))\n",
        "#   input2 = Input(shape=(1,))\n",
        "\n",
        "#   d1 = Dense(np.rint(np.sqrt(out_area)), activation='relu')(input1)\n",
        "#   d2 = Dense(np.rint(np.sqrt(out_area)), activation='relu')(input2)\n",
        "#   merged = concatenate([d1, d2])\n",
        "\n",
        "#   #inputs = concatenate([theta, ro])\n",
        "#   #x = Input(shape=input_shape)\n",
        "#   d1 = Dense(np.rint(np.sqrt(out_area)), activation='relu')(merged)\n",
        "#   #d1 = Dense(out_area//2, activation='relu')(merged)\n",
        "#   d2 = Dense(out_area, activation='softmax')(d1)\n",
        "  \n",
        "#   model = Model(inputs=[input1, input2], outputs=d2)\n",
        "#   print(model.summary())\n",
        "#   return model\n",
        "\n",
        "# def get_network(output_shape=(10,10)):\n",
        "#   out1, out2 = output_shape\n",
        "#   out_area = out1 * out2\n",
        "\n",
        "#   input1 = Input(shape=(1,))\n",
        "#   input2 = Input(shape=(1,))\n",
        "\n",
        "#   u1 = 32\n",
        "\n",
        "#   d1 = Dense(u1)(input1)\n",
        "#   d1 = LeakyReLU()(d1)\n",
        "#   d2 = Dense(u1)(input2)\n",
        "#   d2 = LeakyReLU()(d2)\n",
        "#   merged = concatenate([d1, d2])\n",
        "\n",
        "#   u2 = 128\n",
        "#   d3 = Dense(u2)(merged)\n",
        "#   d3 = LeakyReLU()(d3)\n",
        "#   d3 = Dropout(0.2)(d3)\n",
        "\n",
        "#   d3 = Dense(u2*2)(d3)\n",
        "#   d3 = LeakyReLU()(d3)\n",
        "#   d4 = Dense(out_area, activation='softmax')(d3)\n",
        "  \n",
        "#   model = Model(inputs=[input1, input2], outputs=d4)\n",
        "#   print(model.summary())\n",
        "#   return model\n"
      ],
      "metadata": {
        "id": "iQlY1kqcypBe"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = get_network(output_shape=(10,10))\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 1e-4\n",
        "\n",
        "net.compile(\n",
        "    optimizer=Adam(learning_rate=learning_rate),\n",
        "    loss=CategoricalCrossentropy()\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT52b_0w1Z4R",
        "outputId": "459a6560-b315-47c2-cd6b-dee96afba874"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " input_8 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 2)            0           ['input_7[0][0]',                \n",
            "                                                                  'input_8[0][0]']                \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 64)           192         ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " leaky_re_lu_2 (LeakyReLU)      (None, 64)           0           ['dense_9[0][0]']                \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 64)           4160        ['leaky_re_lu_2[0][0]']          \n",
            "                                                                                                  \n",
            " leaky_re_lu_3 (LeakyReLU)      (None, 64)           0           ['dense_10[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 64)           0           ['leaky_re_lu_3[0][0]']          \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 100)          6500        ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10,852\n",
            "Trainable params: 10,852\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 256\n",
        "\n",
        "callback = EarlyStopping(monitor=\"loss\",\n",
        "    min_delta=0.001,\n",
        "    patience=20,\n",
        "    restore_best_weights=True)\n",
        "\n",
        "# history = net.fit_generator(polar_generator(BATCH_SIZE), steps_per_epoch=1000, epochs=10)\n",
        "history = net.fit(polar_generator(BATCH_SIZE, flat=True), batch_size = BATCH_SIZE, steps_per_epoch=1000, epochs=1000, callbacks=[callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCILuGA53OgB",
        "outputId": "6f3bb657-6339-45c9-b0df-b1ce181029df"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 2.3656\n",
            "Epoch 2/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 1.0724\n",
            "Epoch 3/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.8283\n",
            "Epoch 4/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.7310\n",
            "Epoch 5/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6771\n",
            "Epoch 6/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6401\n",
            "Epoch 7/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6163\n",
            "Epoch 8/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.5925\n",
            "Epoch 9/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5757\n",
            "Epoch 10/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5601\n",
            "Epoch 11/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.5474\n",
            "Epoch 12/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5371\n",
            "Epoch 13/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5255\n",
            "Epoch 14/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.5169\n",
            "Epoch 15/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5058\n",
            "Epoch 16/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5005\n",
            "Epoch 17/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.4939\n",
            "Epoch 18/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4825\n",
            "Epoch 19/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4808\n",
            "Epoch 20/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4730\n",
            "Epoch 21/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4687\n",
            "Epoch 22/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4622\n",
            "Epoch 23/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4557\n",
            "Epoch 24/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4522\n",
            "Epoch 25/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4458\n",
            "Epoch 26/1000\n",
            "1000/1000 [==============================] - 6s 6ms/step - loss: 0.4444\n",
            "Epoch 27/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4393\n",
            "Epoch 28/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4354\n",
            "Epoch 29/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4303\n",
            "Epoch 30/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4270\n",
            "Epoch 31/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4258\n",
            "Epoch 32/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4193\n",
            "Epoch 33/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4174\n",
            "Epoch 34/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4146\n",
            "Epoch 35/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4141\n",
            "Epoch 36/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.4105\n",
            "Epoch 37/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4068\n",
            "Epoch 38/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4059\n",
            "Epoch 39/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.4040\n",
            "Epoch 40/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3973\n",
            "Epoch 41/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3980\n",
            "Epoch 42/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3930\n",
            "Epoch 43/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3898\n",
            "Epoch 44/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3896\n",
            "Epoch 45/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3900\n",
            "Epoch 46/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3852\n",
            "Epoch 47/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3825\n",
            "Epoch 48/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3782\n",
            "Epoch 49/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3811\n",
            "Epoch 50/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3746\n",
            "Epoch 51/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3734\n",
            "Epoch 52/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3737\n",
            "Epoch 53/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3716\n",
            "Epoch 54/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3703\n",
            "Epoch 55/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3693\n",
            "Epoch 56/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3685\n",
            "Epoch 57/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3656\n",
            "Epoch 58/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3618\n",
            "Epoch 59/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3614\n",
            "Epoch 60/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3595\n",
            "Epoch 61/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3615\n",
            "Epoch 62/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3589\n",
            "Epoch 63/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3583\n",
            "Epoch 64/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3583\n",
            "Epoch 65/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3563\n",
            "Epoch 66/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3518\n",
            "Epoch 67/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3560\n",
            "Epoch 68/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3506\n",
            "Epoch 69/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3457\n",
            "Epoch 70/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3509\n",
            "Epoch 71/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3496\n",
            "Epoch 72/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3476\n",
            "Epoch 73/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3451\n",
            "Epoch 74/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3488\n",
            "Epoch 75/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3442\n",
            "Epoch 76/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3433\n",
            "Epoch 77/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3420\n",
            "Epoch 78/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3419\n",
            "Epoch 79/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3402\n",
            "Epoch 80/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3390\n",
            "Epoch 81/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3394\n",
            "Epoch 82/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3375\n",
            "Epoch 83/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3371\n",
            "Epoch 84/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3369\n",
            "Epoch 85/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3364\n",
            "Epoch 86/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3355\n",
            "Epoch 87/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3338\n",
            "Epoch 88/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3330\n",
            "Epoch 89/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3341\n",
            "Epoch 90/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3293\n",
            "Epoch 91/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3275\n",
            "Epoch 92/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3325\n",
            "Epoch 93/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3284\n",
            "Epoch 94/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3285\n",
            "Epoch 95/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3267\n",
            "Epoch 96/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3282\n",
            "Epoch 97/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3228\n",
            "Epoch 98/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3247\n",
            "Epoch 99/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3239\n",
            "Epoch 100/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3206\n",
            "Epoch 101/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3227\n",
            "Epoch 102/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3222\n",
            "Epoch 103/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3201\n",
            "Epoch 104/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3196\n",
            "Epoch 105/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3185\n",
            "Epoch 106/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3197\n",
            "Epoch 107/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3187\n",
            "Epoch 108/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3180\n",
            "Epoch 109/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3166\n",
            "Epoch 110/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3159\n",
            "Epoch 111/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3163\n",
            "Epoch 112/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3150\n",
            "Epoch 113/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3134\n",
            "Epoch 114/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3134\n",
            "Epoch 115/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3107\n",
            "Epoch 116/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3147\n",
            "Epoch 117/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3123\n",
            "Epoch 118/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3095\n",
            "Epoch 119/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3124\n",
            "Epoch 120/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3089\n",
            "Epoch 121/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3086\n",
            "Epoch 122/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3073\n",
            "Epoch 123/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3085\n",
            "Epoch 124/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3076\n",
            "Epoch 125/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3078\n",
            "Epoch 126/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3050\n",
            "Epoch 127/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3082\n",
            "Epoch 128/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3044\n",
            "Epoch 129/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3049\n",
            "Epoch 130/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3048\n",
            "Epoch 131/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3016\n",
            "Epoch 132/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3039\n",
            "Epoch 133/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3042\n",
            "Epoch 134/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3027\n",
            "Epoch 135/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3000\n",
            "Epoch 136/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3007\n",
            "Epoch 137/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3023\n",
            "Epoch 138/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3006\n",
            "Epoch 139/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3021\n",
            "Epoch 140/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2989\n",
            "Epoch 141/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3019\n",
            "Epoch 142/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2968\n",
            "Epoch 143/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2987\n",
            "Epoch 144/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2971\n",
            "Epoch 145/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2968\n",
            "Epoch 146/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2979\n",
            "Epoch 147/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2964\n",
            "Epoch 148/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2951\n",
            "Epoch 149/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2942\n",
            "Epoch 150/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2963\n",
            "Epoch 151/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2939\n",
            "Epoch 152/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2941\n",
            "Epoch 153/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2930\n",
            "Epoch 154/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2930\n",
            "Epoch 155/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2910\n",
            "Epoch 156/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2940\n",
            "Epoch 157/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2931\n",
            "Epoch 158/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2915\n",
            "Epoch 159/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2916\n",
            "Epoch 160/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2880\n",
            "Epoch 161/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2899\n",
            "Epoch 162/1000\n",
            "1000/1000 [==============================] - 6s 6ms/step - loss: 0.2896\n",
            "Epoch 163/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2872\n",
            "Epoch 164/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2884\n",
            "Epoch 165/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2860\n",
            "Epoch 166/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2875\n",
            "Epoch 167/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2882\n",
            "Epoch 168/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2868\n",
            "Epoch 169/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2870\n",
            "Epoch 170/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2860\n",
            "Epoch 171/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2845\n",
            "Epoch 172/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2837\n",
            "Epoch 173/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2857\n",
            "Epoch 174/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2830\n",
            "Epoch 175/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2821\n",
            "Epoch 176/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2825\n",
            "Epoch 177/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2826\n",
            "Epoch 178/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2814\n",
            "Epoch 179/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2837\n",
            "Epoch 180/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2804\n",
            "Epoch 181/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2819\n",
            "Epoch 182/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2775\n",
            "Epoch 183/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2811\n",
            "Epoch 184/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2796\n",
            "Epoch 185/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2786\n",
            "Epoch 186/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2763\n",
            "Epoch 187/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2790\n",
            "Epoch 188/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2781\n",
            "Epoch 189/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2766\n",
            "Epoch 190/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2765\n",
            "Epoch 191/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2754\n",
            "Epoch 192/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2775\n",
            "Epoch 193/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2766\n",
            "Epoch 194/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2739\n",
            "Epoch 195/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2724\n",
            "Epoch 196/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2728\n",
            "Epoch 197/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2757\n",
            "Epoch 198/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2773\n",
            "Epoch 199/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2728\n",
            "Epoch 200/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2734\n",
            "Epoch 201/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2719\n",
            "Epoch 202/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2729\n",
            "Epoch 203/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2719\n",
            "Epoch 204/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2723\n",
            "Epoch 205/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2690\n",
            "Epoch 206/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2720\n",
            "Epoch 207/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2725\n",
            "Epoch 208/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2740\n",
            "Epoch 209/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2720\n",
            "Epoch 210/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2704\n",
            "Epoch 211/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2680\n",
            "Epoch 212/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2704\n",
            "Epoch 213/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2690\n",
            "Epoch 214/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2696\n",
            "Epoch 215/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2699\n",
            "Epoch 216/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2674\n",
            "Epoch 217/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2666\n",
            "Epoch 218/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2664\n",
            "Epoch 219/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2679\n",
            "Epoch 220/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2670\n",
            "Epoch 221/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2680\n",
            "Epoch 222/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2675\n",
            "Epoch 223/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2654\n",
            "Epoch 224/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2638\n",
            "Epoch 225/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2642\n",
            "Epoch 226/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2685\n",
            "Epoch 227/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2647\n",
            "Epoch 228/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2648\n",
            "Epoch 229/1000\n",
            "1000/1000 [==============================] - 6s 6ms/step - loss: 0.2651\n",
            "Epoch 230/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2639\n",
            "Epoch 231/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2656\n",
            "Epoch 232/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2608\n",
            "Epoch 233/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2658\n",
            "Epoch 234/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2629\n",
            "Epoch 235/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2608\n",
            "Epoch 236/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2632\n",
            "Epoch 237/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2616\n",
            "Epoch 238/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2626\n",
            "Epoch 239/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2608\n",
            "Epoch 240/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2596\n",
            "Epoch 241/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2616\n",
            "Epoch 242/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2606\n",
            "Epoch 243/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2619\n",
            "Epoch 244/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2609\n",
            "Epoch 245/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2585\n",
            "Epoch 246/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2586\n",
            "Epoch 247/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2612\n",
            "Epoch 248/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2564\n",
            "Epoch 249/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2612\n",
            "Epoch 250/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2585\n",
            "Epoch 251/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2616\n",
            "Epoch 252/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2599\n",
            "Epoch 253/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2576\n",
            "Epoch 254/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2565\n",
            "Epoch 255/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2582\n",
            "Epoch 256/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2569\n",
            "Epoch 257/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2570\n",
            "Epoch 258/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2569\n",
            "Epoch 259/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2560\n",
            "Epoch 260/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2555\n",
            "Epoch 261/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2556\n",
            "Epoch 262/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2547\n",
            "Epoch 263/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2559\n",
            "Epoch 264/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2527\n",
            "Epoch 265/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2553\n",
            "Epoch 266/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2570\n",
            "Epoch 267/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2544\n",
            "Epoch 268/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2540\n",
            "Epoch 269/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2557\n",
            "Epoch 270/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2535\n",
            "Epoch 271/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2559\n",
            "Epoch 272/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2527\n",
            "Epoch 273/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2554\n",
            "Epoch 274/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2538\n",
            "Epoch 275/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2527\n",
            "Epoch 276/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2520\n",
            "Epoch 277/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2533\n",
            "Epoch 278/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2508\n",
            "Epoch 279/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2528\n",
            "Epoch 280/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2538\n",
            "Epoch 281/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2493\n",
            "Epoch 282/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2502\n",
            "Epoch 283/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2501\n",
            "Epoch 284/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2516\n",
            "Epoch 285/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2483\n",
            "Epoch 286/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2514\n",
            "Epoch 287/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2509\n",
            "Epoch 288/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2517\n",
            "Epoch 289/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2488\n",
            "Epoch 290/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2502\n",
            "Epoch 291/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2515\n",
            "Epoch 292/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2502\n",
            "Epoch 293/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2486\n",
            "Epoch 294/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2483\n",
            "Epoch 295/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2464\n",
            "Epoch 296/1000\n",
            "1000/1000 [==============================] - 6s 6ms/step - loss: 0.2477\n",
            "Epoch 297/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2465\n",
            "Epoch 298/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2474\n",
            "Epoch 299/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2471\n",
            "Epoch 300/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2486\n",
            "Epoch 301/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2470\n",
            "Epoch 302/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2477\n",
            "Epoch 303/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2478\n",
            "Epoch 304/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2442\n",
            "Epoch 305/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2472\n",
            "Epoch 306/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2459\n",
            "Epoch 307/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2477\n",
            "Epoch 308/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2476\n",
            "Epoch 309/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2463\n",
            "Epoch 310/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2450\n",
            "Epoch 311/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2469\n",
            "Epoch 312/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2455\n",
            "Epoch 313/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2490\n",
            "Epoch 314/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2464\n",
            "Epoch 315/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2460\n",
            "Epoch 316/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2461\n",
            "Epoch 317/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2453\n",
            "Epoch 318/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2423\n",
            "Epoch 319/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2441\n",
            "Epoch 320/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2464\n",
            "Epoch 321/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2418\n",
            "Epoch 322/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2430\n",
            "Epoch 323/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2442\n",
            "Epoch 324/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2439\n",
            "Epoch 325/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2435\n",
            "Epoch 326/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2434\n",
            "Epoch 327/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2392\n",
            "Epoch 328/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2428\n",
            "Epoch 329/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2418\n",
            "Epoch 330/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2414\n",
            "Epoch 331/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2403\n",
            "Epoch 332/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2396\n",
            "Epoch 333/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2411\n",
            "Epoch 334/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2422\n",
            "Epoch 335/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2402\n",
            "Epoch 336/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2376\n",
            "Epoch 337/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2394\n",
            "Epoch 338/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2385\n",
            "Epoch 339/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2405\n",
            "Epoch 340/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2367\n",
            "Epoch 341/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2406\n",
            "Epoch 342/1000\n",
            "1000/1000 [==============================] - 6s 6ms/step - loss: 0.2370\n",
            "Epoch 343/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2385\n",
            "Epoch 344/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2379\n",
            "Epoch 345/1000\n",
            "1000/1000 [==============================] - 6s 6ms/step - loss: 0.2369\n",
            "Epoch 346/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2372\n",
            "Epoch 347/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2380\n",
            "Epoch 348/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2381\n",
            "Epoch 349/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2374\n",
            "Epoch 350/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2360\n",
            "Epoch 351/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2374\n",
            "Epoch 352/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2365\n",
            "Epoch 353/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2359\n",
            "Epoch 354/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2345\n",
            "Epoch 355/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2363\n",
            "Epoch 356/1000\n",
            "1000/1000 [==============================] - 6s 6ms/step - loss: 0.2368\n",
            "Epoch 357/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2334\n",
            "Epoch 358/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2346\n",
            "Epoch 359/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2386\n",
            "Epoch 360/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2365\n",
            "Epoch 361/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2344\n",
            "Epoch 362/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2337\n",
            "Epoch 363/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2354\n",
            "Epoch 364/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2334\n",
            "Epoch 365/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2330\n",
            "Epoch 366/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2345\n",
            "Epoch 367/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2317\n",
            "Epoch 368/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2349\n",
            "Epoch 369/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2316\n",
            "Epoch 370/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2317\n",
            "Epoch 371/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2287\n",
            "Epoch 372/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2324\n",
            "Epoch 373/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2302\n",
            "Epoch 374/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2309\n",
            "Epoch 375/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2321\n",
            "Epoch 376/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2308\n",
            "Epoch 377/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2333\n",
            "Epoch 378/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2288\n",
            "Epoch 379/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2322\n",
            "Epoch 380/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2297\n",
            "Epoch 381/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2303\n",
            "Epoch 382/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2305\n",
            "Epoch 383/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2305\n",
            "Epoch 384/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2299\n",
            "Epoch 385/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2309\n",
            "Epoch 386/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2296\n",
            "Epoch 387/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2304\n",
            "Epoch 388/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2306\n",
            "Epoch 389/1000\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2288\n",
            "Epoch 390/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2279\n",
            "Epoch 391/1000\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(net, gen, sample=1000):\n",
        "  (theta,rho),maps = next(gen)\n",
        "  pred = net.predict((theta,rho))\n",
        "\n",
        "  ok = 0\n",
        "  for i in range(sample):\n",
        "    if np.argmax(maps[i]) == np.argmax(pred[i]):\n",
        "      ok += 1\n",
        "\n",
        "  return ok*100/sample"
      ],
      "metadata": {
        "id": "5qz1LZxgAt3c"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy2(net, gen, sample=1000, shape=(10,10)):\n",
        "  (theta,rho),maps = next(gen)\n",
        "  pred = net.predict((theta,rho))\n",
        "\n",
        "  accuracy = 0\n",
        "  \n",
        "  for i in range(sample):\n",
        "    max_true = np.argmax(maps[i])\n",
        "    x_true = max_true//10\n",
        "    y_true = max_true%10\n",
        "    \n",
        "    max_pred = np.argmax(pred[i])\n",
        "    x_pred = max_pred//10\n",
        "    y_pred = max_pred%10\n",
        "\n",
        "    accuracy += (1 - (np.square(x_true - x_pred) + np.square(y_true - y_pred)) / (shape[0]*np.sqrt(2)))\n",
        "\n",
        "  return accuracy*100/sample"
      ],
      "metadata": {
        "id": "5CSwXIPqPOEr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = 50000\n",
        "get_accuracy(net, polar_generator(sample, grid=(10,10), flat=True), sample=sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT4GxEF5S3B8",
        "outputId": "d742e9a5-cb77-4a43-c60e-d4851584e9df"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 2s 1ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95.59"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_accuracy2(net, polar_generator(sample, grid=(10,10), flat=True), sample=sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9Px1BA_EKuU",
        "outputId": "2adcf104-fa71-465d-bf5f-7ad209929844"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 3s 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99.66836691962143"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}