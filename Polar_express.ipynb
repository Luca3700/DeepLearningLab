{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of the project is to learn the mapping from polar coordinates to a a discrete 10x10 grid of cells in the plane, using a neural network. \n",
        "\n",
        "The supervised dataset is given to you in the form of a generator (to be considered as a black box).\n",
        "\n",
        "The model must achieve an accuracy of 95%, and it will be evaluated in a way **inversely proportional to the number of its parameters: the smaller, the better.**\n",
        "\n",
        "**WARNING**: Any solution taking advantage of meta-knowledge about the generator will be automatically rejected."
      ],
      "metadata": {
        "id": "Zw_326KLT9dF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ynz-4_4cFmbJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, concatenate\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the generator. It returns triples of the form ((theta,rho),out) where (theta,rho) are the polar coordinates of a point in the first quadrant of the plane, and out is a 10x10 map with \"1\" in the cell corresponding to the point position, and \"0\" everywhere else.\n",
        "\n",
        "By setting flat=True, the resulting map is flattened into a vector with a single dimension 100. You can use this variant, if you wish. "
      ],
      "metadata": {
        "id": "iA01pkKbUt7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def polar_generator(batchsize,grid=(10,10),noise=.002,flat=False):\n",
        "  while True:\n",
        "    x = np.random.rand(batchsize)\n",
        "    y = np.random.rand(batchsize)\n",
        "    out = np.zeros((batchsize,grid[0],grid[1]))\n",
        "    xc = (x*grid[0]).astype(int)\n",
        "    yc = (y*grid[1]).astype(int)\n",
        "    for b in range(batchsize):\n",
        "      out[b,xc[b],yc[b]] = 1\n",
        "    #compute rho and theta and add some noise\n",
        "    rho = np.sqrt(x**2+y**2) + np.random.normal(scale=noise)\n",
        "    theta = np.arctan(y/np.maximum(x,.00001)) + np.random.normal(scale=noise)\n",
        "    if flat:\n",
        "      out = np.reshape(out,(batchsize,grid[0]*grid[1]))\n",
        "    yield ((theta,rho),out)"
      ],
      "metadata": {
        "id": "DsA1GqAeWAdo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create an instance of the generator on a grid with dimension 3x4"
      ],
      "metadata": {
        "id": "ZF-jlaqAWc2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g1,g2 = 3,4\n",
        "gen = polar_generator(1,grid=(g1,g2),noise=0.0)"
      ],
      "metadata": {
        "id": "Ov3rXaLVHDCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now let's see a few samples."
      ],
      "metadata": {
        "id": "b4hntQtSWjPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(theta,rho),maps = next(gen)\n",
        "for i,map in enumerate(maps):\n",
        "  #let us compute the cartesian coordinates\n",
        "  x = np.cos(theta[i])*rho[i]\n",
        "  y = np.sin(theta[i])*rho[i]\n",
        "  print(\"x coordinate (row): {}\".format(int(x*g1)))\n",
        "  print(\"y coordinate (col): {}\".format(int(y*g2)))\n",
        "  print(\"map:\")\n",
        "  print(np.reshape(map,(g1,g2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PM7R8ZZZHN7p",
        "outputId": "92c37cd5-6f7e-4d40-d57c-c4a132a739d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x coordinate (row): 0\n",
            "y coordinate (col): 1\n",
            "map:\n",
            "[[0. 1. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise: add noise to the generator, and check the effect on the \"ground truth\"."
      ],
      "metadata": {
        "id": "NTY5fu8Hg7RE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What to deliver\n",
        "\n",
        "For the purposes of the project you are supposed to work with the **default 10x10 grid, and the default noise=.002**\n",
        "\n",
        "The generator must be treatead as a black box, do not tweak it, and do not exploit its semantics that is supposed to be unknown. You are allowed to work with the \"flat\" modality, if you prefer so.\n",
        "\n",
        "You need to:\n",
        "1.   define an accuracy function (take inspiration from the code of the previous cell)\n",
        "2.   define a neural network taking in input theta and rho, and returning out\n",
        "3. measure the network's accuracy that must be above 95% (accuracy must be evaluated over at least 20000 samples)\n",
        "4. tune the network trying to decrease as much as possible the numer of parameters, preserving an accuracy above 95%. Only your best network must be delivered.\n",
        "\n",
        "You must deliver a SINGLE notebook working on colab, containing the code of the network, its summary, the training history, the code for the accurary metric and its evaluation on the network.\n",
        "\n",
        "**N.B.** The accuracy must be above 95% but apart from that it does not influence the evaluation. You score will only depend on the number of parameters: the lower, the better.\n",
        "\n",
        "#Good work!\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jj4akvA24maJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.experimental import AdamW\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import ReLU, LeakyReLU, Dropout, BatchNormalization, Add"
      ],
      "metadata": {
        "id": "_pjE0IGoyVW-"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "m59ZUx61SnLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_unit(n: int):\n",
        "  res = 1\n",
        "  while(2^res < n):\n",
        "    res += 1\n",
        "  return res"
      ],
      "metadata": {
        "id": "0oiK7MDZpVaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_network_old(output_shape=(10,10)):\n",
        "  #theta, ro = theta_ro\n",
        "  out1, out2 = output_shape\n",
        "  out_area = out1 * out2\n",
        "\n",
        "  input1 = Input(shape=(1,))\n",
        "  input2 = Input(shape=(1,))\n",
        "  merged = concatenate([input1, input2])\n",
        "\n",
        "  d1 = Dense(48)(merged)\n",
        "  d1 = LeakyReLU()(d1)\n",
        "  d1 = Dense(32)(d1)\n",
        "  d1 = LeakyReLU()(d1)\n",
        "  d1 = Dropout(0.1)(d1)\n",
        "  d2 = Dense(out_area, activation='softmax')(d1)\n",
        "  \n",
        "  model = Model(inputs=[input1, input2], outputs=d2)\n",
        "  print(model.summary())\n",
        "  return model\n",
        "\n"
      ],
      "metadata": {
        "id": "iQlY1kqcypBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New network\n",
        "The input is a tuple containing theta and rho. Rho is processed by two Dense layer (with two units) to rescale the input, instead theta is processed two times in parallel by two Dense layer each time (this is done in order to extract the sin and the cosin). Then we merge the new theta with the sin and the cosin, and those tensors are processed in parallel by two stack of two Dense layer (each having 10 units) in order to extract informations about the x and the y axis. Then those outputs are merged together and the information is processed by a stack of Dense layer composed by two layers having 20 units and a last layer of 100 units to produce the output.\n",
        "The activaction function used are all ReLUs, but in the last layer is used the softmax to obtain the probability that in each grid of the map there will be a 1."
      ],
      "metadata": {
        "id": "xTFM_Hs-Sfty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_network(output_shape=(10,10), num_loop=2, units_preprocessing=2):\n",
        "  out1, out2 = output_shape\n",
        "  out_area = out1 * out2\n",
        "\n",
        "  input1 = Input(shape=(1,))\n",
        "  input2 = Input(shape=(1,))\n",
        "\n",
        "  # processing the first input\n",
        "  x1 = Dense(units_preprocessing)(input1)\n",
        "  #x1 = BatchNormalization()(x1)\n",
        "  x1 = ReLU()(x1)\n",
        "  x1 = Dense(units_preprocessing)(x1)\n",
        "  #x1 = BatchNormalization()(x1)\n",
        "  x1 = ReLU()(x1)\n",
        "\n",
        "  # processing in parallel again the first input\n",
        "  x2 = Dense(units_preprocessing)(input1)\n",
        "  #x2 = BatchNormalization()(x2)\n",
        "  x2 = ReLU()(x2)\n",
        "  x2 = Dense(units_preprocessing)(x2)\n",
        "  #x2 = BatchNormalization()(x2)\n",
        "  x2 = ReLU()(x2)\n",
        "\n",
        "  # processing the second input\n",
        "  x3 = Dense(units_preprocessing)(input2)\n",
        "  #x3 = BatchNormalization()(x3)\n",
        "  x3 = ReLU()(x3)\n",
        "  x3 = Dense(units_preprocessing)(x3)\n",
        "  #x3 = BatchNormalization()(x3)\n",
        "  x3 = ReLU()(x3)\n",
        "\n",
        "  # merge the first and the second input\n",
        "  merged1 = concatenate([x1, x3])\n",
        "  merged2 = concatenate([x2, x3])\n",
        "  d1 = merged1\n",
        "  d2 = merged2\n",
        "\n",
        "  # process in parallel those tensors\n",
        "  for i in range(num_loop):\n",
        "    d1 = Dense(out1)(d1)\n",
        "    #d1 = BatchNormalization()(d1)\n",
        "    d1 = ReLU()(d1)\n",
        "\n",
        "    d2 = Dense(out2)(d2)\n",
        "    #d2 = BatchNormalization()(d2)\n",
        "    d2 = ReLU()(d2)\n",
        "\n",
        "  # merge the outputs  \n",
        "  merged3 = concatenate([d1,d2])\n",
        "  d3 = merged3\n",
        "\n",
        "  for i in range(num_loop):\n",
        "    d3 = Dense(out1+out2)(d3)\n",
        "    #d3 = BatchNormalization()(d3)\n",
        "    d3 = ReLU()(d3)\n",
        "\n",
        "  output_layer = Dense(out_area, activation='softmax')(d3)\n",
        "  \n",
        "  model = Model(inputs=[input1, input2], outputs=output_layer)\n",
        "  print(model.summary())\n",
        "  return model"
      ],
      "metadata": {
        "id": "Hzwr3Y_mmiCq"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = get_network(output_shape=(10,10))\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 1e-4\n",
        "\n",
        "net.compile(\n",
        "    optimizer=Adam(learning_rate=learning_rate),\n",
        "    loss=CategoricalCrossentropy()\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT52b_0w1Z4R",
        "outputId": "e793ad08-ce76-48c8-ee7f-55241dc4f90d"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_35\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_77 (InputLayer)          [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " input_78 (InputLayer)          [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_507 (Dense)              (None, 2)            4           ['input_77[0][0]']               \n",
            "                                                                                                  \n",
            " dense_511 (Dense)              (None, 2)            4           ['input_78[0][0]']               \n",
            "                                                                                                  \n",
            " dense_509 (Dense)              (None, 2)            4           ['input_77[0][0]']               \n",
            "                                                                                                  \n",
            " re_lu_468 (ReLU)               (None, 2)            0           ['dense_507[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_472 (ReLU)               (None, 2)            0           ['dense_511[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_470 (ReLU)               (None, 2)            0           ['dense_509[0][0]']              \n",
            "                                                                                                  \n",
            " dense_508 (Dense)              (None, 2)            6           ['re_lu_468[0][0]']              \n",
            "                                                                                                  \n",
            " dense_512 (Dense)              (None, 2)            6           ['re_lu_472[0][0]']              \n",
            "                                                                                                  \n",
            " dense_510 (Dense)              (None, 2)            6           ['re_lu_470[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_469 (ReLU)               (None, 2)            0           ['dense_508[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_473 (ReLU)               (None, 2)            0           ['dense_512[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_471 (ReLU)               (None, 2)            0           ['dense_510[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_116 (Concatenate)  (None, 4)            0           ['re_lu_469[0][0]',              \n",
            "                                                                  're_lu_473[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_117 (Concatenate)  (None, 4)            0           ['re_lu_471[0][0]',              \n",
            "                                                                  're_lu_473[0][0]']              \n",
            "                                                                                                  \n",
            " dense_513 (Dense)              (None, 10)           50          ['concatenate_116[0][0]']        \n",
            "                                                                                                  \n",
            " dense_514 (Dense)              (None, 10)           50          ['concatenate_117[0][0]']        \n",
            "                                                                                                  \n",
            " re_lu_474 (ReLU)               (None, 10)           0           ['dense_513[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_475 (ReLU)               (None, 10)           0           ['dense_514[0][0]']              \n",
            "                                                                                                  \n",
            " dense_515 (Dense)              (None, 10)           110         ['re_lu_474[0][0]']              \n",
            "                                                                                                  \n",
            " dense_516 (Dense)              (None, 10)           110         ['re_lu_475[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_476 (ReLU)               (None, 10)           0           ['dense_515[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_477 (ReLU)               (None, 10)           0           ['dense_516[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_118 (Concatenate)  (None, 20)           0           ['re_lu_476[0][0]',              \n",
            "                                                                  're_lu_477[0][0]']              \n",
            "                                                                                                  \n",
            " dense_517 (Dense)              (None, 20)           420         ['concatenate_118[0][0]']        \n",
            "                                                                                                  \n",
            " re_lu_478 (ReLU)               (None, 20)           0           ['dense_517[0][0]']              \n",
            "                                                                                                  \n",
            " dense_518 (Dense)              (None, 20)           420         ['re_lu_478[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_479 (ReLU)               (None, 20)           0           ['dense_518[0][0]']              \n",
            "                                                                                                  \n",
            " dense_519 (Dense)              (None, 100)          2100        ['re_lu_479[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,290\n",
            "Trainable params: 3,290\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 2048\n",
        "\n",
        "callback = EarlyStopping(monitor=\"loss\",\n",
        "    min_delta=0.0005,\n",
        "    patience=20,\n",
        "    restore_best_weights=True)\n",
        "\n",
        "# history = net.fit_generator(polar_generator(BATCH_SIZE), steps_per_epoch=1000, epochs=10)\n",
        "history = net.fit(polar_generator(BATCH_SIZE, flat=True), batch_size = BATCH_SIZE, steps_per_epoch=1000, epochs=1000, callbacks=[callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCILuGA53OgB",
        "outputId": "4c1ed95e-ea47-4b45-9f91-6f78973e4902"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "1000/1000 [==============================] - 14s 12ms/step - loss: 1.6976\n",
            "Epoch 2/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4227\n",
            "Epoch 3/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2881\n",
            "Epoch 4/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.2341\n",
            "Epoch 5/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.2062\n",
            "Epoch 6/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1917\n",
            "Epoch 7/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1823\n",
            "Epoch 8/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1714\n",
            "Epoch 9/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1655\n",
            "Epoch 10/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1592\n",
            "Epoch 11/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1541\n",
            "Epoch 12/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1537\n",
            "Epoch 13/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1495\n",
            "Epoch 14/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1492\n",
            "Epoch 15/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1446\n",
            "Epoch 16/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1456\n",
            "Epoch 17/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1443\n",
            "Epoch 18/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1439\n",
            "Epoch 19/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1413\n",
            "Epoch 20/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1411\n",
            "Epoch 21/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1388\n",
            "Epoch 22/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1379\n",
            "Epoch 23/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1391\n",
            "Epoch 24/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1363\n",
            "Epoch 25/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1361\n",
            "Epoch 26/1000\n",
            "1000/1000 [==============================] - 15s 15ms/step - loss: 0.1362\n",
            "Epoch 27/1000\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1353\n",
            "Epoch 28/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1339\n",
            "Epoch 29/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1305\n",
            "Epoch 30/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1330\n",
            "Epoch 31/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1339\n",
            "Epoch 32/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1301\n",
            "Epoch 33/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1287\n",
            "Epoch 34/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1347\n",
            "Epoch 35/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1335\n",
            "Epoch 36/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1345\n",
            "Epoch 37/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1306\n",
            "Epoch 38/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1354\n",
            "Epoch 39/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1303\n",
            "Epoch 40/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1268\n",
            "Epoch 41/1000\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1319\n",
            "Epoch 42/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1249\n",
            "Epoch 43/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1322\n",
            "Epoch 44/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1271\n",
            "Epoch 45/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1281\n",
            "Epoch 46/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1280\n",
            "Epoch 47/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1323\n",
            "Epoch 48/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1242\n",
            "Epoch 49/1000\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1299\n",
            "Epoch 50/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1372\n",
            "Epoch 51/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1319\n",
            "Epoch 52/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1251\n",
            "Epoch 53/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1277\n",
            "Epoch 54/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1332\n",
            "Epoch 55/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1218\n",
            "Epoch 56/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1280\n",
            "Epoch 57/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1323\n",
            "Epoch 58/1000\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1274\n",
            "Epoch 59/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1293\n",
            "Epoch 60/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1247\n",
            "Epoch 61/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1330\n",
            "Epoch 62/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1220\n",
            "Epoch 63/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1287\n",
            "Epoch 64/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1197\n",
            "Epoch 65/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1217\n",
            "Epoch 66/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1266\n",
            "Epoch 67/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1266\n",
            "Epoch 68/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1296\n",
            "Epoch 69/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1243\n",
            "Epoch 70/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1256\n",
            "Epoch 71/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1254\n",
            "Epoch 72/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1274\n",
            "Epoch 73/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1224\n",
            "Epoch 74/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1270\n",
            "Epoch 75/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1335\n",
            "Epoch 76/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1150\n",
            "Epoch 77/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1275\n",
            "Epoch 78/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1254\n",
            "Epoch 79/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1207\n",
            "Epoch 80/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1219\n",
            "Epoch 81/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1254\n",
            "Epoch 82/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1231\n",
            "Epoch 83/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1305\n",
            "Epoch 84/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1246\n",
            "Epoch 85/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1216\n",
            "Epoch 86/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1272\n",
            "Epoch 87/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1212\n",
            "Epoch 88/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1287\n",
            "Epoch 89/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1305\n",
            "Epoch 90/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1190\n",
            "Epoch 91/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1214\n",
            "Epoch 92/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1244\n",
            "Epoch 93/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1259\n",
            "Epoch 94/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1292\n",
            "Epoch 95/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1217\n",
            "Epoch 96/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(net, gen, sample=1000):\n",
        "  (theta,rho),maps = next(gen)\n",
        "  pred = net.predict((theta,rho))\n",
        "\n",
        "  ok = 0\n",
        "  for i in range(sample):\n",
        "    if np.argmax(maps[i]) == np.argmax(pred[i]):\n",
        "      ok += 1\n",
        "\n",
        "  return ok*100/sample"
      ],
      "metadata": {
        "id": "5qz1LZxgAt3c"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy2(net, gen, sample=1000, shape=(10,10)):\n",
        "  (theta,rho),maps = next(gen)\n",
        "  pred = net.predict((theta,rho))\n",
        "\n",
        "  accuracy = 0\n",
        "  \n",
        "  for i in range(sample):\n",
        "    max_true = np.argmax(maps[i])\n",
        "    x_true = max_true//10\n",
        "    y_true = max_true%10\n",
        "    \n",
        "    max_pred = np.argmax(pred[i])\n",
        "    x_pred = max_pred//10\n",
        "    y_pred = max_pred%10\n",
        "\n",
        "    accuracy += (1 - (np.square(x_true - x_pred) + np.square(y_true - y_pred)) / (shape[0]*np.sqrt(2)))\n",
        "\n",
        "  return accuracy*100/sample"
      ],
      "metadata": {
        "id": "5CSwXIPqPOEr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = int(10e4)\n",
        "get_accuracy(net, polar_generator(sample, grid=(10,10), flat=True), sample=sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT4GxEF5S3B8",
        "outputId": "2fb31032-6ca7-4e24-df8e-3f58016fdc64"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3125/3125 [==============================] - 5s 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96.577"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_accuracy2(net, polar_generator(sample, grid=(10,10), flat=True), sample=sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9Px1BA_EKuU",
        "outputId": "e52d8929-28fd-4a9a-fb91-0a2e941e6c4e"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3125/3125 [==============================] - 6s 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99.65330554518182"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    }
  ]
}