{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of the project is to learn the mapping from polar coordinates to a a discrete 10x10 grid of cells in the plane, using a neural network. \n",
        "\n",
        "The supervised dataset is given to you in the form of a generator (to be considered as a black box).\n",
        "\n",
        "The model must achieve an accuracy of 95%, and it will be evaluated in a way **inversely proportional to the number of its parameters: the smaller, the better.**\n",
        "\n",
        "**WARNING**: Any solution taking advantage of meta-knowledge about the generator will be automatically rejected."
      ],
      "metadata": {
        "id": "Zw_326KLT9dF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ynz-4_4cFmbJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, concatenate\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the generator. It returns triples of the form ((theta,rho),out) where (theta,rho) are the polar coordinates of a point in the first quadrant of the plane, and out is a 10x10 map with \"1\" in the cell corresponding to the point position, and \"0\" everywhere else.\n",
        "\n",
        "By setting flat=True, the resulting map is flattened into a vector with a single dimension 100. You can use this variant, if you wish. "
      ],
      "metadata": {
        "id": "iA01pkKbUt7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def polar_generator(batchsize,grid=(10,10),noise=.002,flat=False):\n",
        "  while True:\n",
        "    x = np.random.rand(batchsize)\n",
        "    y = np.random.rand(batchsize)\n",
        "    out = np.zeros((batchsize,grid[0],grid[1]))\n",
        "    xc = (x*grid[0]).astype(int)\n",
        "    yc = (y*grid[1]).astype(int)\n",
        "    for b in range(batchsize):\n",
        "      out[b,xc[b],yc[b]] = 1\n",
        "    #compute rho and theta and add some noise\n",
        "    rho = np.sqrt(x**2+y**2) + np.random.normal(scale=noise)\n",
        "    theta = np.arctan(y/np.maximum(x,.00001)) + np.random.normal(scale=noise)\n",
        "    if flat:\n",
        "      out = np.reshape(out,(batchsize,grid[0]*grid[1]))\n",
        "    yield ((theta,rho),out)"
      ],
      "metadata": {
        "id": "DsA1GqAeWAdo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create an instance of the generator on a grid with dimension 3x4"
      ],
      "metadata": {
        "id": "ZF-jlaqAWc2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g1,g2 = 3,4\n",
        "gen = polar_generator(1,grid=(g1,g2),noise=0.0)"
      ],
      "metadata": {
        "id": "Ov3rXaLVHDCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now let's see a few samples."
      ],
      "metadata": {
        "id": "b4hntQtSWjPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(theta,rho),maps = next(gen)\n",
        "for i,map in enumerate(maps):\n",
        "  #let us compute the cartesian coordinates\n",
        "  x = np.cos(theta[i])*rho[i]\n",
        "  y = np.sin(theta[i])*rho[i]\n",
        "  print(\"x coordinate (row): {}\".format(int(x*g1)))\n",
        "  print(\"y coordinate (col): {}\".format(int(y*g2)))\n",
        "  print(\"map:\")\n",
        "  print(np.reshape(map,(g1,g2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PM7R8ZZZHN7p",
        "outputId": "92c37cd5-6f7e-4d40-d57c-c4a132a739d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x coordinate (row): 0\n",
            "y coordinate (col): 1\n",
            "map:\n",
            "[[0. 1. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise: add noise to the generator, and check the effect on the \"ground truth\"."
      ],
      "metadata": {
        "id": "NTY5fu8Hg7RE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What to deliver\n",
        "\n",
        "For the purposes of the project you are supposed to work with the **default 10x10 grid, and the default noise=.002**\n",
        "\n",
        "The generator must be treatead as a black box, do not tweak it, and do not exploit its semantics that is supposed to be unknown. You are allowed to work with the \"flat\" modality, if you prefer so.\n",
        "\n",
        "You need to:\n",
        "1.   define an accuracy function (take inspiration from the code of the previous cell)\n",
        "2.   define a neural network taking in input theta and rho, and returning out\n",
        "3. measure the network's accuracy that must be above 95% (accuracy must be evaluated over at least 20000 samples)\n",
        "4. tune the network trying to decrease as much as possible the numer of parameters, preserving an accuracy above 95%. Only your best network must be delivered.\n",
        "\n",
        "You must deliver a SINGLE notebook working on colab, containing the code of the network, its summary, the training history, the code for the accurary metric and its evaluation on the network.\n",
        "\n",
        "**N.B.** The accuracy must be above 95% but apart from that it does not influence the evaluation. You score will only depend on the number of parameters: the lower, the better.\n",
        "\n",
        "#Good work!\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jj4akvA24maJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.experimental import AdamW\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import ReLU, LeakyReLU, Dropout"
      ],
      "metadata": {
        "id": "_pjE0IGoyVW-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "m59ZUx61SnLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_unit(n: int):\n",
        "  res = 1\n",
        "  while(2^res < n):\n",
        "    res += 1\n",
        "  return res"
      ],
      "metadata": {
        "id": "0oiK7MDZpVaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_network_old(output_shape=(10,10)):\n",
        "  #theta, ro = theta_ro\n",
        "  out1, out2 = output_shape\n",
        "  out_area = out1 * out2\n",
        "\n",
        "  input1 = Input(shape=(1,))\n",
        "  input2 = Input(shape=(1,))\n",
        "  merged = concatenate([input1, input2])\n",
        "\n",
        "  d1 = Dense(48)(merged)\n",
        "  d1 = LeakyReLU()(d1)\n",
        "  d1 = Dense(32)(d1)\n",
        "  d1 = LeakyReLU()(d1)\n",
        "  d1 = Dropout(0.1)(d1)\n",
        "  d2 = Dense(out_area, activation='softmax')(d1)\n",
        "  \n",
        "  model = Model(inputs=[input1, input2], outputs=d2)\n",
        "  print(model.summary())\n",
        "  return model\n",
        "\n"
      ],
      "metadata": {
        "id": "iQlY1kqcypBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New network\n",
        "The input is a tuple containing theta and rho. Rho is processed by a single Dense layer (with a single unit) to rescale the input, instead theta is processed two times in parallel by two Dense layer each time (this is done in order to extract the sin and the cosin). Then we merge the new theta with the sin and the cosin, and those layers are processed in parallel by two stack of two Dense layer (each having 10 units) in order to extract informations about the x and the y axis. Then those outputs are merged together and the information is processed by a stack of Dense layer composed by two layers having 20 units and a last layer of 100 units to produce the output.\n",
        "The activaction function used are all ReLUs, but in the last layer is used a sigmoid to obtain the probability that in each grid of the map there will be a 1."
      ],
      "metadata": {
        "id": "xTFM_Hs-Sfty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_network(output_shape=(10,10)):\n",
        "  #theta, ro = theta_ro\n",
        "  out1, out2 = output_shape\n",
        "  out_area = out1 * out2\n",
        "\n",
        "  input1 = Input(shape=(1,))\n",
        "  input2 = Input(shape=(1,))\n",
        "\n",
        "  x1 = Dense(1)(input1)\n",
        "  x1 = ReLU()(x1)\n",
        "  x1 = Dense(1)(x1)\n",
        "  x1 = ReLU()(x1)\n",
        "\n",
        "  x2 = Dense(1)(input1)\n",
        "  x2 = ReLU()(x2)\n",
        "  x2 = Dense(1)(x2)\n",
        "  x2 = ReLU()(x2)\n",
        "\n",
        "  x3 = Dense(1)(input2)\n",
        "  x3 = ReLU()(x3)\n",
        "\n",
        "  merged1 = concatenate([x1, x3])\n",
        "  merged2 = concatenate([x2, x3])\n",
        "\n",
        "  d1 = Dense(10)(merged1)\n",
        "  d1 = ReLU()(d1)\n",
        "  d1 = Dense(10)(d1)\n",
        "  d1 = ReLU()(d1)\n",
        "  d2 = Dense(10)(merged2)\n",
        "  d2 = ReLU()(d2)\n",
        "  d2 = Dense(10)(d2)\n",
        "  d2 = ReLU()(d2)\n",
        "  \n",
        "  merged3 = concatenate([d1,d2])\n",
        "  d3 = Dense(20)(merged3)\n",
        "  d3 = ReLU()(d3)\n",
        "  d3 = Dense(20)(d3)\n",
        "  d3 = ReLU()(d3)\n",
        "  \n",
        "  d3 = Dense(out_area, activation='softmax')(d3)\n",
        "  \n",
        "  model = Model(inputs=[input1, input2], outputs=d3)\n",
        "  print(model.summary())\n",
        "  return model"
      ],
      "metadata": {
        "id": "wLneiXoCGkjs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = get_network(output_shape=(10,10))\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 1e-4\n",
        "\n",
        "net.compile(\n",
        "    optimizer=Adam(learning_rate=learning_rate),\n",
        "    loss=CategoricalCrossentropy()\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT52b_0w1Z4R",
        "outputId": "cecf74c3-5665-4fb7-cdde-3d408be7a517"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_15 (InputLayer)          [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_63 (Dense)               (None, 1)            2           ['input_15[0][0]']               \n",
            "                                                                                                  \n",
            " dense_65 (Dense)               (None, 1)            2           ['input_15[0][0]']               \n",
            "                                                                                                  \n",
            " re_lu_56 (ReLU)                (None, 1)            0           ['dense_63[0][0]']               \n",
            "                                                                                                  \n",
            " input_16 (InputLayer)          [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " re_lu_58 (ReLU)                (None, 1)            0           ['dense_65[0][0]']               \n",
            "                                                                                                  \n",
            " dense_64 (Dense)               (None, 1)            2           ['re_lu_56[0][0]']               \n",
            "                                                                                                  \n",
            " dense_67 (Dense)               (None, 1)            2           ['input_16[0][0]']               \n",
            "                                                                                                  \n",
            " dense_66 (Dense)               (None, 1)            2           ['re_lu_58[0][0]']               \n",
            "                                                                                                  \n",
            " re_lu_57 (ReLU)                (None, 1)            0           ['dense_64[0][0]']               \n",
            "                                                                                                  \n",
            " re_lu_60 (ReLU)                (None, 1)            0           ['dense_67[0][0]']               \n",
            "                                                                                                  \n",
            " re_lu_59 (ReLU)                (None, 1)            0           ['dense_66[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_21 (Concatenate)   (None, 2)            0           ['re_lu_57[0][0]',               \n",
            "                                                                  're_lu_60[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_22 (Concatenate)   (None, 2)            0           ['re_lu_59[0][0]',               \n",
            "                                                                  're_lu_60[0][0]']               \n",
            "                                                                                                  \n",
            " dense_68 (Dense)               (None, 10)           30          ['concatenate_21[0][0]']         \n",
            "                                                                                                  \n",
            " dense_70 (Dense)               (None, 10)           30          ['concatenate_22[0][0]']         \n",
            "                                                                                                  \n",
            " re_lu_61 (ReLU)                (None, 10)           0           ['dense_68[0][0]']               \n",
            "                                                                                                  \n",
            " re_lu_63 (ReLU)                (None, 10)           0           ['dense_70[0][0]']               \n",
            "                                                                                                  \n",
            " dense_69 (Dense)               (None, 10)           110         ['re_lu_61[0][0]']               \n",
            "                                                                                                  \n",
            " dense_71 (Dense)               (None, 10)           110         ['re_lu_63[0][0]']               \n",
            "                                                                                                  \n",
            " re_lu_62 (ReLU)                (None, 10)           0           ['dense_69[0][0]']               \n",
            "                                                                                                  \n",
            " re_lu_64 (ReLU)                (None, 10)           0           ['dense_71[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_23 (Concatenate)   (None, 20)           0           ['re_lu_62[0][0]',               \n",
            "                                                                  're_lu_64[0][0]']               \n",
            "                                                                                                  \n",
            " dense_72 (Dense)               (None, 20)           420         ['concatenate_23[0][0]']         \n",
            "                                                                                                  \n",
            " re_lu_65 (ReLU)                (None, 20)           0           ['dense_72[0][0]']               \n",
            "                                                                                                  \n",
            " dense_73 (Dense)               (None, 20)           420         ['re_lu_65[0][0]']               \n",
            "                                                                                                  \n",
            " re_lu_66 (ReLU)                (None, 20)           0           ['dense_73[0][0]']               \n",
            "                                                                                                  \n",
            " dense_74 (Dense)               (None, 100)          2100        ['re_lu_66[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,230\n",
            "Trainable params: 3,230\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 2048\n",
        "\n",
        "callback = EarlyStopping(monitor=\"loss\",\n",
        "    min_delta=0.0005,\n",
        "    patience=20,\n",
        "    restore_best_weights=True)\n",
        "\n",
        "# history = net.fit_generator(polar_generator(BATCH_SIZE), steps_per_epoch=1000, epochs=10)\n",
        "history = net.fit(polar_generator(BATCH_SIZE, flat=True), batch_size = BATCH_SIZE, steps_per_epoch=1000, epochs=1000, callbacks=[callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCILuGA53OgB",
        "outputId": "e4d630fb-e7dc-404f-cf02-504201b5973c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "1000/1000 [==============================] - 15s 12ms/step - loss: 2.9679\n",
            "Epoch 2/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 1.9547\n",
            "Epoch 3/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.9172\n",
            "Epoch 4/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.5613\n",
            "Epoch 5/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4060\n",
            "Epoch 6/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3345\n",
            "Epoch 7/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2939\n",
            "Epoch 8/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2671\n",
            "Epoch 9/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2476\n",
            "Epoch 10/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2344\n",
            "Epoch 11/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2207\n",
            "Epoch 12/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2106\n",
            "Epoch 13/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2036\n",
            "Epoch 14/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1941\n",
            "Epoch 15/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1812\n",
            "Epoch 16/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1791\n",
            "Epoch 17/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1741\n",
            "Epoch 18/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1727\n",
            "Epoch 19/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1669\n",
            "Epoch 20/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1656\n",
            "Epoch 21/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1647\n",
            "Epoch 22/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1554\n",
            "Epoch 23/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1571\n",
            "Epoch 24/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1566\n",
            "Epoch 25/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1511\n",
            "Epoch 26/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1493\n",
            "Epoch 27/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1508\n",
            "Epoch 28/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1515\n",
            "Epoch 29/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1477\n",
            "Epoch 30/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1462\n",
            "Epoch 31/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1454\n",
            "Epoch 32/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1409\n",
            "Epoch 33/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1422\n",
            "Epoch 34/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1437\n",
            "Epoch 35/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1436\n",
            "Epoch 36/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1377\n",
            "Epoch 37/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1372\n",
            "Epoch 38/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1408\n",
            "Epoch 39/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1395\n",
            "Epoch 40/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1385\n",
            "Epoch 41/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1325\n",
            "Epoch 42/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1387\n",
            "Epoch 43/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1347\n",
            "Epoch 44/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1342\n",
            "Epoch 45/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1356\n",
            "Epoch 46/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1363\n",
            "Epoch 47/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1340\n",
            "Epoch 48/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1281\n",
            "Epoch 49/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1308\n",
            "Epoch 50/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1298\n",
            "Epoch 51/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1334\n",
            "Epoch 52/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1328\n",
            "Epoch 53/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1311\n",
            "Epoch 54/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1289\n",
            "Epoch 55/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1311\n",
            "Epoch 56/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1345\n",
            "Epoch 57/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1298\n",
            "Epoch 58/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1343\n",
            "Epoch 59/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1300\n",
            "Epoch 60/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1252\n",
            "Epoch 61/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1294\n",
            "Epoch 62/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1303\n",
            "Epoch 63/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1278\n",
            "Epoch 64/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1249\n",
            "Epoch 65/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1323\n",
            "Epoch 66/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1308\n",
            "Epoch 67/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1278\n",
            "Epoch 68/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1233\n",
            "Epoch 69/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1239\n",
            "Epoch 70/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1299\n",
            "Epoch 71/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1300\n",
            "Epoch 72/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1277\n",
            "Epoch 73/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1245\n",
            "Epoch 74/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1295\n",
            "Epoch 75/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1284\n",
            "Epoch 76/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1230\n",
            "Epoch 77/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1238\n",
            "Epoch 78/1000\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1247\n",
            "Epoch 79/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1245\n",
            "Epoch 80/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1260\n",
            "Epoch 81/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1238\n",
            "Epoch 82/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1284\n",
            "Epoch 83/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1261\n",
            "Epoch 84/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1262\n",
            "Epoch 85/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1252\n",
            "Epoch 86/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1261\n",
            "Epoch 87/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1247\n",
            "Epoch 88/1000\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(net, gen, sample=1000):\n",
        "  (theta,rho),maps = next(gen)\n",
        "  pred = net.predict((theta,rho))\n",
        "\n",
        "  ok = 0\n",
        "  for i in range(sample):\n",
        "    if np.argmax(maps[i]) == np.argmax(pred[i]):\n",
        "      ok += 1\n",
        "\n",
        "  return ok*100/sample"
      ],
      "metadata": {
        "id": "5qz1LZxgAt3c"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy2(net, gen, sample=1000, shape=(10,10)):\n",
        "  (theta,rho),maps = next(gen)\n",
        "  pred = net.predict((theta,rho))\n",
        "\n",
        "  accuracy = 0\n",
        "  \n",
        "  for i in range(sample):\n",
        "    max_true = np.argmax(maps[i])\n",
        "    x_true = max_true//10\n",
        "    y_true = max_true%10\n",
        "    \n",
        "    max_pred = np.argmax(pred[i])\n",
        "    x_pred = max_pred//10\n",
        "    y_pred = max_pred%10\n",
        "\n",
        "    accuracy += (1 - (np.square(x_true - x_pred) + np.square(y_true - y_pred)) / (shape[0]*np.sqrt(2)))\n",
        "\n",
        "  return accuracy*100/sample"
      ],
      "metadata": {
        "id": "5CSwXIPqPOEr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = int(10e5)\n",
        "get_accuracy(net, polar_generator(sample, grid=(10,10), flat=True), sample=sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT4GxEF5S3B8",
        "outputId": "5f48e977-b04e-4126-87c7-cfe53139fd10"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31250/31250 [==============================] - 53s 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93.1657"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_accuracy2(net, polar_generator(sample, grid=(10,10), flat=True), sample=sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9Px1BA_EKuU",
        "outputId": "5501de33-16e6-4b27-b24b-9a5e8c75e6c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 2s 1ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99.70485362953092"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}